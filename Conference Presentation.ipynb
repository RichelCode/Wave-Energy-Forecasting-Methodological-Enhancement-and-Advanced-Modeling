{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0433a37-9c12-45b5-b7ec-496695dfbedd",
   "metadata": {},
   "source": [
    "## WAVE ENERGY FORECASTING: A RIGOROUS MACHINE LEARNING APPROACH\n",
    "\n",
    "Project: Advanced Wave Energy Flux Forecasting Using Ensemble Methods and \n",
    "         Deep Learning with Uncertainty Quantification\n",
    "Target Station: NDBC Buoy 41025 (Diamond Shoals, NC)\n",
    "\n",
    "Repository: https://github.com/RichelCode/Wave-Energy-Forecasting-Methodological-Enhancement-and-Advanced-Modeling\n",
    "\n",
    "Paper Reference: Building upon \"A Hybrid Machine Learning Approach to Wave \n",
    "                 Energy Forecasting\" (TIefu et al., 2021, NAPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ca481-95e0-4b83-a2e8-ccdb9268a38c",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This notebook implements a comprehensive wave energy forecasting system that addresses critical methodological limitations in existing literature while advancing toward state-of-the-art performance. Our work builds upon and significantly improves the methodology presented in TIefu et al. (2021), incorporating:\n",
    "\n",
    "Key Improvements:\n",
    "- Rigorous time series cross-validation (eliminating data leakage)\n",
    "- Spatial imputation using neighbor station data (preserving temporal continuity)\n",
    "- Comprehensive feature engineering with temporal lags and domain knowledge\n",
    "- Systematic hyperparameter optimization using Bayesian methods\n",
    "- Statistical significance testing for model comparisons\n",
    "- Neural network baselines (LSTM, GRU) alongside tree-based methods\n",
    "- Probabilistic forecasting with uncertainty quantification\n",
    "- Multi-horizon evaluation (1h, 3h, 6h, 12h, 24h ahead)\n",
    "\n",
    "## Problem Context and Motivation\n",
    "\n",
    "###  The Wave Energy Challenge\n",
    "\n",
    "Wave energy represents a **largely untapped renewable resource** with estimated potential of:\n",
    "- **1,170 TWh/year** along US coastlines (EPRI, 2011)\n",
    "- **2-3 TW global theoretical capacity** (Gunn & Stock-Williams, 2012)\n",
    "- **Predictable and high energy density** compared to wind/solar\n",
    "\n",
    "However, **grid integration of wave energy** requires accurate forecasting for:\n",
    "\n",
    "1. **Power System Operations:**\n",
    "   - Unit commitment and economic dispatch optimization\n",
    "   - Spinning reserve allocation\n",
    "   - Transmission congestion management\n",
    "   - Ancillary service procurement\n",
    "\n",
    "2. **Wave Energy Converter (WEC) Management:**\n",
    "   - Preventive maintenance scheduling during calm periods\n",
    "   - Survival mode activation before extreme events\n",
    "   - Optimal power extraction strategies\n",
    "   - Component lifetime optimization\n",
    "\n",
    "3. **Market Participation:**\n",
    "   - Day-ahead and real-time market bidding\n",
    "   - Financial risk management\n",
    "   - Revenue optimization under uncertainty\n",
    "\n",
    "### Critical Forecasting Parameters\n",
    "\n",
    "Wave energy flux (kW/m) depends primarily on:\n",
    "\n",
    "**Significant Wave Height (H_s):** \n",
    "- The mean height of the highest 1/3 of waves in a time period\n",
    "- Directly measured by buoy accelerometers\n",
    "- Energy ∝ H_s² (quadratic relationship amplifies forecast errors)\n",
    "\n",
    "**Wave Period (T):**\n",
    "- Dominant Period (DPD): Period of peak energy in wave spectrum\n",
    "- Average Period (APD): Mean period of all waves\n",
    "- Energy ∝ T (linear relationship but crucial for power capture)\n",
    "\n",
    "**Wave Energy Flux Formula:**\n",
    "$$E = \\frac{\\rho g^2 H_s^2 T}{64\\pi} \\approx 0.49 H_s^2 T \\text{ (kW/m in deep water)}$$\n",
    "\n",
    "Where:\n",
    "- ρ = seawater density ≈ 1025 kg/m³\n",
    "- g = gravitational acceleration = 9.81 m/s²\n",
    "- H_s = significant wave height (m)\n",
    "- T = wave period (s)\n",
    "\n",
    "**Implication:** A 10% error in H_s translates to ~20% error in energy flux due to quadratic relationship. This demands high-accuracy forecasting.\n",
    "\n",
    "\n",
    "## Literature Review and Research Gaps\n",
    "\n",
    "### Existing Approaches\n",
    "\n",
    "**Physics-Based Models:**\n",
    "- WAVEWATCH III, SWAN (Simulating WAves Nearshore)\n",
    "- **Pros:** Based on first principles, global applicability\n",
    "- **Cons:** Computationally expensive (hours for 24h forecast), require accurate wind forcing, cumulative error from physics approximations\n",
    "\n",
    "**Statistical/ML Models:**\n",
    "- ARIMA, SARIMA for time series\n",
    "- Neural Networks (ANN, LSTM, GRU)\n",
    "- Tree-based methods (Random Forest, XGBoost)\n",
    "- **Pros:** Fast inference, learn from data patterns\n",
    "- **Cons:** Site-specific training, require substantial historical data\n",
    "\n",
    "**Hybrid Models:**\n",
    "- Decomposition + ML (VMD-LSTM, EMD-XGBoost)\n",
    "- Stacking ensembles combining multiple algorithms\n",
    "- Physics-informed neural networks\n",
    "- **Pros:** Leverage complementary strengths\n",
    "- **Cons:** Increased complexity, hyperparameter tuning challenges\n",
    "\n",
    "### Identified Research Gaps (TIefu et al., 2021)\n",
    "\n",
    "After thorough review of the foundational paper, we identified **critical methodological deficiencies:**\n",
    "\n",
    "| Issue | Impact | Our Solution |\n",
    "|-------|--------|--------------|\n",
    "| Single train-test split | Overfitting risk, no robustness assessment | Time series cross-validation |\n",
    "| No hyperparameter tuning | Unfair model comparison | Bayesian optimization (Optuna) |\n",
    "| Missing neural network baselines | Incomplete various model comparison | LSTM, GRU, TCN (Temporal Convolutional Neural Network) implementations |\n",
    "| Simple outlier deletion | Data loss, temporal gaps | Spatial imputation from neighbor station |\n",
    "| No temporal lags | Limited forecasting capability | Lag features [1, 3, 6, 12, 24, 48h] |\n",
    "| No statistical testing | Uncertain significance of improvements | Paired t-tests, DM tests, bootstrap CI |\n",
    "| Point predictions only | No uncertainty quantification | Quantile regression, conformal prediction |\n",
    "| Single horizon evaluation | Limited operational utility | Multi-horizon [1, 3, 6, 12, 24h] |\n",
    "\n",
    "**Our work systematically addresses each of these gaps.**\n",
    "\n",
    "\n",
    "## Research Objectives and Hypotheses\n",
    "\n",
    "### Primary Objectives\n",
    "\n",
    "**Objective 1:** Develop a methodologically rigorous wave energy forecasting framework that eliminates common pitfalls in applied ML research (data leakage, inadequate validation, missing baselines).\n",
    "\n",
    "**Objective 2:** Systematically compare tree-based ensemble methods (XGBoost, Random Forest) and deep learning architectures (LSTM, GRU, TCN) under fair experimental conditions with proper hyperparameter tuning.\n",
    "\n",
    "**Objective 3:** Design and validate a heterogeneous ensemble approach that combines complementary model strengths while avoiding redundancy.\n",
    "\n",
    "**Objective 4:** Implement probabilistic forecasting to quantify prediction uncertainty for operational decision-making.\n",
    "\n",
    "**Objective 5:** Evaluate performance across multiple forecast horizons (1-24 hours) and wave energy regimes (low, moderate, high, extreme).\n",
    "\n",
    "\n",
    "## Data Sources and Study Site\n",
    "\n",
    "### Target Location: Diamond Shoals, NC (NDBC Station 41025)\n",
    "\n",
    "**Geographic Characteristics:**\n",
    "- **Coordinates:** 35.006°N, 75.402°W\n",
    "- **Location:** 15 nautical miles southeast of Cape Hatteras, North Carolina\n",
    "- **Water Depth:** 48.8 meters (deep water conditions)\n",
    "- **Exposure:** Open Atlantic Ocean, high wave energy environment\n",
    "- **Wave Climate:** Mixed sea and swell, strong seasonal variability\n",
    "\n",
    "**Strategic Importance:**\n",
    "- Located in the **U.S. Mid-Atlantic wave energy hotspot**\n",
    "- Representative of conditions for proposed WEC deployments\n",
    "- Long historical record (1990s-present) for robust modeling\n",
    "- Well-maintained instrument with high data quality\n",
    "\n",
    "**Why This Location?**\n",
    "- Cape Hatteras region has **average wave power >30 kW/m** (top 10% of US sites)\n",
    "- Proximity to population centers (Virginia, North Carolina coast)\n",
    "- Overlaps with proposed offshore wind farms (potential co-location)\n",
    "- Existing grid infrastructure for renewable integration\n",
    "\n",
    "### Data Description\n",
    "\n",
    "**Source:** National Data Buoy Center (NDBC), NOAA\n",
    "**Temporal Coverage:** January 2014 - December 2019 (6 years)\n",
    "**Temporal Resolution:** Hourly measurements\n",
    "**Total Potential Records:** 52,608 hours\n",
    "\n",
    "**Variables Available:**\n",
    "\n",
    "| Variable | Symbol | Unit | Description | Role in Analysis |\n",
    "|----------|--------|------|-------------|------------------|\n",
    "| Significant Wave Height | WVHT | meters | Mean height of highest 1/3 waves | **Primary Target** |\n",
    "| Dominant Wave Period | DPD | seconds | Period at spectral peak | **Primary Target** |\n",
    "| Average Wave Period | APD | seconds | Mean wave period | **Secondary Target/Feature** |\n",
    "| Mean Wave Direction | MWD | degrees | Direction waves coming from | Feature |\n",
    "| Wind Direction | WDIR | degrees | Direction wind coming from | Feature |\n",
    "| Wind Speed | WSPD | m/s | 10-minute average wind speed | Feature |\n",
    "| Wind Gust | GST | m/s | Peak 5-second wind speed | Feature |\n",
    "| Atmospheric Pressure | PRES | hPa | Sea-level pressure | Feature |\n",
    "| Air Temperature | ATMP | °C | Air temperature | Feature |\n",
    "| Sea Surface Temperature | WTMP | °C | Water temperature at 1m depth | Feature |\n",
    "\n",
    "**Derived Targets:**\n",
    "- **Wave Energy Flux** (kW/m): Calculated from WVHT and DPD using deep water approximation\n",
    "- **Wave Power Production** (kW): Theoretical power capture assuming 30% WEC efficiency\n",
    "\n",
    "### Data Quality Considerations\n",
    "\n",
    "**Known Issues (from NDBC documentation):**\n",
    "- Missing data codes: 999, 9999, 99.0 (varies by variable)\n",
    "- Sensor failures during extreme weather events (non-random missingness)\n",
    "- Occasional calibration gaps during maintenance periods\n",
    "- Wave direction uncertainty during low wave conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584925c-2b54-4a0f-ba98-a907681686e1",
   "metadata": {},
   "source": [
    "# DATA LOADING AND INITIAL ASSESSMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687397-b580-4c6e-bdd7-c8cd66742501",
   "metadata": {},
   "source": [
    "## Understanding NDBC Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d9d26d-853d-4ec8-9970-567b0d93b427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDBC Data Format Documentation\n",
      "================================================================================\n",
      "✓ Yearly text files with space-separated values\n",
      "✓ First two rows contain headers and units\n",
      "✓ Missing data coded as 999, 9999, 99, or MM\n",
      "✓ Hourly temporal resolution (typically at :00 or :50 minutes)\n",
      "================================================================================\n",
      "NDBC Data Format Documentation\n",
      "================================================================================\n",
      "✓ Yearly text files with space-separated values\n",
      "✓ First two rows contain headers and units\n",
      "✓ Missing data coded as 999, 9999, 99, or MM\n",
      "✓ Hourly temporal resolution (typically at :00 or :50 minutes)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NDBC Standard Meteorological Data Format (from NDBC documentation):\n",
    "\n",
    "The data files contain hourly observations with the following characteristics:\n",
    "\n",
    "FILE STRUCTURE:\n",
    "- Line 1: Column headers (variable names)\n",
    "- Line 2: Unit specifications  \n",
    "- Line 3+: Hourly data records\n",
    "\n",
    "MISSING DATA CODES:\n",
    "- 999  or 999.0  → Missing wave/wind data\n",
    "- 9999 or 9999.0 → Missing pressure data  \n",
    "- 99   or 99.0   → Missing temperature data\n",
    "- MM   → Missing month/time component\n",
    "\n",
    "COMMON VARIABLES:\n",
    "#YY  = Year (2014-2019 in our case)\n",
    "MM   = Month (1-12)\n",
    "DD   = Day (1-31)\n",
    "hh   = Hour (0-23)\n",
    "mm   = Minute (usually 00 or 50)\n",
    "WDIR = Wind direction (degrees, meteorological convention)\n",
    "WSPD = Wind speed (m/s)\n",
    "GST  = Wind gust (m/s)\n",
    "WVHT = Significant wave height (meters)\n",
    "DPD  = Dominant wave period (seconds)\n",
    "APD  = Average wave period (seconds)  \n",
    "MWD  = Mean wave direction (degrees)\n",
    "PRES = Sea level pressure (hPa)\n",
    "ATMP = Air temperature (°C)\n",
    "WTMP = Sea surface temperature (°C)\n",
    "\"\"\"\n",
    "\n",
    "print(\"NDBC Data Format Documentation\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Yearly text files with space-separated values\")\n",
    "print(\"✓ First two rows contain headers and units\")\n",
    "print(\"✓ Missing data coded as 999, 9999, 99, or MM\")\n",
    "print(\"✓ Hourly temporal resolution (typically at :00 or :50 minutes)\")\n",
    "print(\"=\" * 80)\n",
    "\"\"\"\n",
    "NDBC Standard Meteorological Data Format (from NDBC documentation):\n",
    "\n",
    "The data files contain hourly observations with the following characteristics:\n",
    "\n",
    "FILE STRUCTURE:\n",
    "- Line 1: Column headers (variable names)\n",
    "- Line 2: Unit specifications  \n",
    "- Line 3+: Hourly data records\n",
    "\n",
    "MISSING DATA CODES:\n",
    "- 999  or 999.0  → Missing wave/wind data\n",
    "- 9999 or 9999.0 → Missing pressure data  \n",
    "- 99   or 99.0   → Missing temperature data\n",
    "- MM   → Missing month/time component\n",
    "\n",
    "COMMON VARIABLES:\n",
    "#YY  = Year (2014-2019 in our case)\n",
    "MM   = Month (1-12)\n",
    "DD   = Day (1-31)\n",
    "hh   = Hour (0-23)\n",
    "mm   = Minute (usually 00 or 50)\n",
    "WDIR = Wind direction (degrees, meteorological convention)\n",
    "WSPD = Wind speed (m/s)\n",
    "GST  = Wind gust (m/s)\n",
    "WVHT = Significant wave height (meters)\n",
    "DPD  = Dominant wave period (seconds)\n",
    "APD  = Average wave period (seconds)  \n",
    "MWD  = Mean wave direction (degrees)\n",
    "PRES = Sea level pressure (hPa)\n",
    "ATMP = Air temperature (°C)\n",
    "WTMP = Sea surface temperature (°C)\n",
    "\"\"\"\n",
    "\n",
    "print(\"NDBC Data Format Documentation\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Yearly text files with space-separated values\")\n",
    "print(\"✓ First two rows contain headers and units\")\n",
    "print(\"✓ Missing data coded as 999, 9999, 99, or MM\")\n",
    "print(\"✓ Hourly temporal resolution (typically at :00 or :50 minutes)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9829054-4a25-4e5a-9e67-febff55ffcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as n\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b20caa-83e1-46fc-8c0b-13caf57680aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard NDBC missing-value codes\n",
    "na_vals = [99, 99.0, 999, 999.0, 9999, 9999.0, \"MM\"]\n",
    "\n",
    "\n",
    "def read_ndbc_stdmet(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a single NDBC standard meteorological file.\n",
    "\n",
    "    - Finds the '#YY MM DD hh mm ...' header line\n",
    "    - Uses that as column names\n",
    "    - Treats NDBC missing codes as NaN\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        header = None\n",
    "        for line in f:\n",
    "            if line.startswith(\"#YY\") or line.startswith(\"YY\"):\n",
    "                header = line.lstrip(\"#\").strip().split()\n",
    "                break\n",
    "\n",
    "    if header is None:\n",
    "        raise ValueError(f\"Couldn't find #YY header in {path}\")\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=r\"\\s+\",\n",
    "        comment=\"#\",\n",
    "        header=None,\n",
    "        names=header,\n",
    "        na_values=na_vals,\n",
    "        engine=\"python\",\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184fed2f-667c-4d74-b5a1-d36e8faa8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_station_df(station_prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a single DataFrame for a station from yearly NDBC files.\n",
    "\n",
    "    Example:\n",
    "      station_prefix = '41025h'  -> matches 41025h2014.txt, 41025h2015.txt, ...\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(f\"{station_prefix}*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found matching pattern {station_prefix}*.txt\")\n",
    "\n",
    "    print(\"Found files:\")\n",
    "    for f in files:\n",
    "        print(\"  \", os.path.basename(f))\n",
    "\n",
    "    dfs = [read_ndbc_stdmet(f) for f in files]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Standardize time columns\n",
    "    col_map = {\n",
    "        \"#YY\": \"year\",\n",
    "        \"YY\": \"year\",\n",
    "        \"MM\": \"month\",\n",
    "        \"DD\": \"day\",\n",
    "        \"hh\": \"hour\",\n",
    "        \"mm\": \"minute\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n",
    "\n",
    "    # Build datetime index\n",
    "    time_cols = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "    df[\"datetime\"] = pd.to_datetime(df[time_cols], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"datetime\"]).set_index(\"datetime\").sort_index()\n",
    "    df = df.drop(columns=[c for c in time_cols if c in df.columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f2f41f-38ae-484f-9aba-738f6ace0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      "C:\\Users\\attafuro\\Desktop\\Wave Analysis\n",
      "\n",
      "Files in this directory:\n",
      "   .ipynb_checkpoints\n",
      "   41025h2014.txt\n",
      "   41025h2015.txt\n",
      "   41025h2016.txt\n",
      "   41025h2017.txt\n",
      "   41025h2018.txt\n",
      "   41025h2019.txt\n",
      "   44095h2014.txt\n",
      "   44095h2015.txt\n",
      "   44095h2016.txt\n",
      "   44095h2017.txt\n",
      "   44095h2018.txt\n",
      "   44095h2019.txt\n",
      "   APD_LSTM_predictions.xlsx\n",
      "   APD_predictions.xlsx\n",
      "   APD_results.xlsx\n",
      "   Conference Presentation.ipynb\n",
      "   Imputing Neighbor Stations .ipynb\n",
      "   Imputing with Neighbor Stations.ipynb\n",
      "   Imputing with traditional method.ipynb\n",
      "   WVHT_LSTM_predictions.xlsx\n",
      "   WVHT_T_Predictions_with_Neighbor.xlsx\n",
      "   WVHT_predictions.xlsx\n",
      "   Wave Analysis.ipynb\n",
      "   Wave_Energy_Flux.xlsx\n",
      "   Wave_Energy_Flux_Metrics.xlsx\n",
      "   Wave_Energy_Flux_TimeSeries.xlsx\n",
      "   Wave_analysis_Considering_Two_Stations.ipynb\n",
      "\n",
      "Searching for files with pattern: 41025h*.txt\n",
      "Found 6 file(s):\n",
      "  • 41025h2014.txt        (  597.5 KB)\n",
      "  • 41025h2015.txt        (  616.6 KB)\n",
      "  • 41025h2016.txt        (  755.9 KB)\n",
      "  • 41025h2017.txt        ( 1306.5 KB)\n",
      "  • 41025h2018.txt        ( 4376.7 KB)\n",
      "  • 41025h2019.txt        ( 2679.5 KB)\n",
      "Found files:\n",
      "   41025h2014.txt\n",
      "   41025h2015.txt\n",
      "   41025h2016.txt\n",
      "   41025h2017.txt\n",
      "   41025h2018.txt\n",
      "   41025h2019.txt\n",
      "\n",
      "Combined Station 41025 DataFrame:\n",
      "------------------------------------------------------------\n",
      "Index range: 2013-12-31 23:50:00 → 2019-12-31 23:50:00\n",
      "Total rows: 117882\n",
      "Columns: ['WDIR', 'WSPD', 'GST', 'WVHT', 'DPD', 'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'TIDE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-31 23:50:00</th>\n",
       "      <td>305.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.56</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1025.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>23.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:50:00</th>\n",
       "      <td>301.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6.25</td>\n",
       "      <td>4.40</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1026.6</td>\n",
       "      <td>11.8</td>\n",
       "      <td>23.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:50:00</th>\n",
       "      <td>305.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5.88</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1027.6</td>\n",
       "      <td>11.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:50:00</th>\n",
       "      <td>308.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5.88</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>23.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 03:50:00</th>\n",
       "      <td>322.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.25</td>\n",
       "      <td>4.38</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      WDIR  WSPD   GST  WVHT   DPD   APD    MWD    PRES  ATMP  \\\n",
       "datetime                                                                        \n",
       "2013-12-31 23:50:00  305.0   7.0  10.0  0.96  5.56  4.41    2.0  1025.5  11.8   \n",
       "2014-01-01 00:50:00  301.0   6.8   9.5  0.96  6.25  4.40  360.0  1026.6  11.8   \n",
       "2014-01-01 01:50:00  305.0   7.5   9.5  0.89  5.88  4.23    2.0  1027.6  11.9   \n",
       "2014-01-01 02:50:00  308.0   6.6   9.4  0.89  5.88  4.15    3.0  1027.9  11.7   \n",
       "2014-01-01 03:50:00  322.0   7.0  10.1  0.92  6.25  4.38   26.0  1027.9  12.0   \n",
       "\n",
       "                     WTMP  DEWP  VIS  TIDE  \n",
       "datetime                                    \n",
       "2013-12-31 23:50:00  23.3   1.8  NaN   NaN  \n",
       "2014-01-01 00:50:00  23.3   1.4  NaN   NaN  \n",
       "2014-01-01 01:50:00  23.3   0.5  NaN   NaN  \n",
       "2014-01-01 02:50:00  23.3  -0.1  NaN   NaN  \n",
       "2014-01-01 03:50:00  23.2  -0.3  NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:10:00</th>\n",
       "      <td>266.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1008.9</td>\n",
       "      <td>15.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:20:00</th>\n",
       "      <td>263.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:30:00</th>\n",
       "      <td>271.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>11.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>7.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:40:00</th>\n",
       "      <td>277.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10.81</td>\n",
       "      <td>5.44</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1009.1</td>\n",
       "      <td>15.7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:50:00</th>\n",
       "      <td>275.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      WDIR  WSPD   GST  WVHT    DPD   APD   MWD    PRES  ATMP  \\\n",
       "datetime                                                                        \n",
       "2019-12-31 23:10:00  266.0   8.3  10.7   NaN    NaN   NaN   NaN  1008.9  15.5   \n",
       "2019-12-31 23:20:00  263.0   8.9  11.2   NaN    NaN   NaN   NaN  1009.0  15.5   \n",
       "2019-12-31 23:30:00  271.0   8.6  11.5   NaN    NaN   NaN   NaN  1009.2  15.7   \n",
       "2019-12-31 23:40:00  277.0   8.8  12.1  1.25  10.81  5.44  20.0  1009.1  15.7   \n",
       "2019-12-31 23:50:00  275.0   9.4  12.6   NaN    NaN   NaN   NaN  1009.2  15.7   \n",
       "\n",
       "                     WTMP  DEWP  VIS  TIDE  \n",
       "datetime                                    \n",
       "2019-12-31 23:10:00  23.1   7.0  NaN   NaN  \n",
       "2019-12-31 23:20:00  23.1   7.0  NaN   NaN  \n",
       "2019-12-31 23:30:00  22.9   7.6  NaN   NaN  \n",
       "2019-12-31 23:40:00  23.0   6.6  NaN   NaN  \n",
       "2019-12-31 23:50:00  23.0   6.3  NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Current working directory:\")\n",
    "print(os.getcwd())\n",
    "print(\"\\nFiles in this directory:\")\n",
    "for f in sorted(os.listdir()):\n",
    "    print(\"  \", f)\n",
    "\n",
    "# Pattern for your files: 41025h2014.txt, 41025h2015.txt, ...\n",
    "file_pattern = \"41025h*.txt\"\n",
    "\n",
    "data_files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "print(\"\\nSearching for files with pattern:\", file_pattern)\n",
    "print(f\"Found {len(data_files)} file(s):\")\n",
    "for f in data_files:\n",
    "    size_kb = os.path.getsize(f) / 1024\n",
    "    print(f\"  • {os.path.basename(f):20s}  ({size_kb:7.1f} KB)\")\n",
    "\n",
    "if len(data_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No files found matching {file_pattern}. \"\n",
    "        \"If needed, change the working directory or file_pattern.\"\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Build the combined DataFrame for Station 41025\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "df_41025 = build_station_df(\"41025h\")\n",
    "\n",
    "print(\"\\nCombined Station 41025 DataFrame:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Index range:\", df_41025.index.min(), \"→\", df_41025.index.max())\n",
    "print(\"Total rows:\", len(df_41025))\n",
    "print(\"Columns:\", df_41025.columns.tolist())\n",
    "\n",
    "# Show a quick preview\n",
    "display(df_41025.head())\n",
    "display(df_41025.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e8dddd-c52f-4a6f-b63a-69b859110717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime index summary\n",
      "----------------------------------------\n",
      "Start: 2013-12-31 23:50:00\n",
      "End  : 2019-12-31 23:50:00\n",
      "Rows : 117882\n",
      "\n",
      "Time step statistics (hours):\n",
      "count    117881.000000\n",
      "mean          0.446077\n",
      "std          12.281906\n",
      "min           0.166667\n",
      "25%           0.166667\n",
      "50%           0.166667\n",
      "75%           1.000000\n",
      "max        3881.166667\n",
      "Name: datetime, dtype: float64\n",
      "\n",
      "Core variables present: ['WVHT', 'DPD', 'APD', 'WSPD', 'GST']\n",
      "\n",
      "% missing per core variable:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>missing_%</th>\n",
       "      <td>62.441255</td>\n",
       "      <td>62.441255</td>\n",
       "      <td>62.441255</td>\n",
       "      <td>0.92126</td>\n",
       "      <td>0.923805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                WVHT        DPD        APD     WSPD       GST\n",
       "missing_%  62.441255  62.441255  62.441255  0.92126  0.923805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUICK SANITY CHECKS ON STATION 41025 DATA\n",
    "# ============================================================================\n",
    "print(\"Datetime index summary\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Start:\", df_41025.index.min())\n",
    "print(\"End  :\", df_41025.index.max())\n",
    "print(\"Rows :\", len(df_41025))\n",
    "\n",
    "# Check spacing between timestamps (should be mostly 1 hour)\n",
    "dt_diff = df_41025.index.to_series().diff().dropna()\n",
    "dt_hours = dt_diff.dt.total_seconds() / 3600\n",
    "\n",
    "print(\"\\nTime step statistics (hours):\")\n",
    "print(dt_hours.describe())\n",
    "\n",
    "# Core variables we care about\n",
    "core_vars = [v for v in [\"WVHT\", \"DPD\", \"APD\", \"WSPD\", \"GST\"] if v in df_41025.columns]\n",
    "print(\"\\nCore variables present:\", core_vars)\n",
    "\n",
    "missing_summary = (\n",
    "    df_41025[core_vars]\n",
    "    .isna()\n",
    "    .mean()\n",
    "    .mul(100)\n",
    "    .rename(\"missing_%\")\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "print(\"\\n% missing per core variable:\")\n",
    "display(missing_summary.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d8c371-da21-4458-b6b0-e31909919e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GAP ANALYSIS: WVHT ===\n",
      "Number of gaps ≥ 6 hours: 6\n",
      "Top 5 longest gaps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_starts = is_missing & ~is_missing.shift(1).fillna(False)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_ends = is_missing & ~is_missing.shift(-1).fillna(False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-21 06:50:00</td>\n",
       "      <td>2019-06-01 00:30:00</td>\n",
       "      <td>3882.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-22 10:50:00</td>\n",
       "      <td>2019-06-24 12:30:00</td>\n",
       "      <td>50.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-07 18:30:00</td>\n",
       "      <td>2017-11-09 19:30:00</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-17 16:50:00</td>\n",
       "      <td>2018-12-18 19:30:00</td>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-12 07:50:00</td>\n",
       "      <td>2014-03-12 20:50:00</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start                 end  duration_hours\n",
       "3 2018-12-21 06:50:00 2019-06-01 00:30:00     3882.666667\n",
       "5 2019-06-22 10:50:00 2019-06-24 12:30:00       50.666667\n",
       "1 2017-11-07 18:30:00 2017-11-09 19:30:00       50.000000\n",
       "2 2018-12-17 16:50:00 2018-12-18 19:30:00       27.666667\n",
       "0 2014-03-12 07:50:00 2014-03-12 20:50:00       14.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GAP ANALYSIS: DPD ===\n",
      "Number of gaps ≥ 6 hours: 6\n",
      "Top 5 longest gaps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_starts = is_missing & ~is_missing.shift(1).fillna(False)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_ends = is_missing & ~is_missing.shift(-1).fillna(False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-21 06:50:00</td>\n",
       "      <td>2019-06-01 00:30:00</td>\n",
       "      <td>3882.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-22 10:50:00</td>\n",
       "      <td>2019-06-24 12:30:00</td>\n",
       "      <td>50.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-07 18:30:00</td>\n",
       "      <td>2017-11-09 19:30:00</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-17 16:50:00</td>\n",
       "      <td>2018-12-18 19:30:00</td>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-12 07:50:00</td>\n",
       "      <td>2014-03-12 20:50:00</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start                 end  duration_hours\n",
       "3 2018-12-21 06:50:00 2019-06-01 00:30:00     3882.666667\n",
       "5 2019-06-22 10:50:00 2019-06-24 12:30:00       50.666667\n",
       "1 2017-11-07 18:30:00 2017-11-09 19:30:00       50.000000\n",
       "2 2018-12-17 16:50:00 2018-12-18 19:30:00       27.666667\n",
       "0 2014-03-12 07:50:00 2014-03-12 20:50:00       14.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GAP ANALYSIS: APD ===\n",
      "Number of gaps ≥ 6 hours: 6\n",
      "Top 5 longest gaps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_starts = is_missing & ~is_missing.shift(1).fillna(False)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\4142490280.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gap_ends = is_missing & ~is_missing.shift(-1).fillna(False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-21 06:50:00</td>\n",
       "      <td>2019-06-01 00:30:00</td>\n",
       "      <td>3882.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-22 10:50:00</td>\n",
       "      <td>2019-06-24 12:30:00</td>\n",
       "      <td>50.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-07 18:30:00</td>\n",
       "      <td>2017-11-09 19:30:00</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-17 16:50:00</td>\n",
       "      <td>2018-12-18 19:30:00</td>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-12 07:50:00</td>\n",
       "      <td>2014-03-12 20:50:00</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start                 end  duration_hours\n",
       "3 2018-12-21 06:50:00 2019-06-01 00:30:00     3882.666667\n",
       "5 2019-06-22 10:50:00 2019-06-24 12:30:00       50.666667\n",
       "1 2017-11-07 18:30:00 2017-11-09 19:30:00       50.000000\n",
       "2 2018-12-17 16:50:00 2018-12-18 19:30:00       27.666667\n",
       "0 2014-03-12 07:50:00 2014-03-12 20:50:00       14.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1.4 GAP ANALYSIS FOR CRITICAL VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "def identify_gaps(series, min_gap_hours=6):\n",
    "    \"\"\"\n",
    "    Identify gaps (consecutive NaNs) of at least min_gap_hours in a time series.\n",
    "    \"\"\"\n",
    "    is_missing = series.isna()\n",
    "    gap_starts = is_missing & ~is_missing.shift(1).fillna(False)\n",
    "    gap_ends = is_missing & ~is_missing.shift(-1).fillna(False)\n",
    "\n",
    "    gaps_info = []\n",
    "    for start, end in zip(series.index[gap_starts], series.index[gap_ends]):\n",
    "        duration = (end - start).total_seconds() / 3600 + 1\n",
    "        if duration >= min_gap_hours:\n",
    "            gaps_info.append(\n",
    "                {\"start\": start, \"end\": end, \"duration_hours\": duration}\n",
    "            )\n",
    "\n",
    "    gaps_df = pd.DataFrame(gaps_info)\n",
    "    if len(gaps_df) > 0:\n",
    "        gaps_df = gaps_df.sort_values(\"duration_hours\", ascending=False)\n",
    "    return gaps_df\n",
    "\n",
    "\n",
    "critical_vars = [v for v in [\"WVHT\", \"DPD\", \"APD\"] if v in df_41025.columns]\n",
    "\n",
    "for var in critical_vars:\n",
    "    print(f\"\\n=== GAP ANALYSIS: {var} ===\")\n",
    "    gaps_df = identify_gaps(df_41025[var], min_gap_hours=6)\n",
    "    if len(gaps_df) == 0:\n",
    "        print(\"No gaps ≥ 6 hours.\")\n",
    "    else:\n",
    "        print(f\"Number of gaps ≥ 6 hours: {len(gaps_df)}\")\n",
    "        print(\"Top 5 longest gaps:\")\n",
    "        display(gaps_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfb1c3-1a71-4867-8520-dfe82dfa1b43",
   "metadata": {},
   "source": [
    "## Final Dataset: 2019–2024 (41025 with neighbour 41001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ff8118-e544-49e0-b4e6-6e44ac40265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:\n",
      "   41025h2014.txt\n",
      "   41025h2015.txt\n",
      "   41025h2016.txt\n",
      "   41025h2017.txt\n",
      "   41025h2018.txt\n",
      "   41025h2019.txt\n",
      "   41025h2020.txt\n",
      "   41025h2021.txt\n",
      "   41025h2022.txt\n",
      "   41025h2023.txt\n",
      "   41025h2024.txt\n",
      "Found files:\n",
      "   41001h2019.txt\n",
      "   41001h2020.txt\n",
      "   41001h2021.txt\n",
      "   41001h2022.txt\n",
      "   41001h2023.txt\n",
      "   41001h2024.txt\n",
      "41025 2019–2024: 2019-06-01 00:00:00 → 2024-12-31 23:50:00 278376\n",
      "41001 2019–2024: 2019-05-01 18:40:00 → 2024-09-20 06:00:00 193706\n"
     ]
    }
   ],
   "source": [
    "# Rebuild full dataframes (if not already in memory)\n",
    "df_41025_full = build_station_df(\"41025h\")\n",
    "df_41001_full = build_station_df(\"41001h\")\n",
    "\n",
    "# Restrict to 2019–2024\n",
    "START_DATE = \"2019-01-01\"\n",
    "END_DATE   = \"2024-12-31\"\n",
    "\n",
    "df_41025 = df_41025_full.loc[START_DATE:END_DATE].copy()\n",
    "df_41001 = df_41001_full.loc[START_DATE:END_DATE].copy()\n",
    "\n",
    "print(\"41025 2019–2024:\", df_41025.index.min(), \"→\", df_41025.index.max(), len(df_41025))\n",
    "print(\"41001 2019–2024:\", df_41001.index.min(), \"→\", df_41001.index.max(), len(df_41001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1056daa-63c8-49fe-b4b9-06d5f8cd76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned 2019–2024 dataset (common timestamps only):\n",
      "  shape     : (187186, 26)\n",
      "  date range: 2019-06-01 00:00:00 → 2024-09-20 06:00:00 \n",
      "\n",
      "Correlation between 41025 and 41001 (2019–2024, overlapping valid data):\n",
      "\n",
      "Checking pair: WVHT_41025 vs WVHT_41001\n",
      "  Rows with both non-missing: 26436\n",
      "  Pearson  r = 0.732, p = 0.0e+00\n",
      "  Spearman r = 0.726, p = 0.0e+00\n",
      "\n",
      "Checking pair: DPD_41025 vs DPD_41001\n",
      "  Rows with both non-missing: 26436\n",
      "  Pearson  r = 0.577, p = 0.0e+00\n",
      "  Spearman r = 0.520, p = 0.0e+00\n",
      "\n",
      "Checking pair: APD_41025 vs APD_41001\n",
      "  Rows with both non-missing: 26436\n",
      "  Pearson  r = 0.718, p = 0.0e+00\n",
      "  Spearman r = 0.639, p = 0.0e+00\n",
      "\n",
      "Checking pair: WSPD_41025 vs WSPD_41001\n",
      "  Rows with both non-missing: 186793\n",
      "  Pearson  r = 0.616, p = 0.0e+00\n",
      "  Spearman r = 0.592, p = 0.0e+00\n",
      "\n",
      "\n",
      "Summary table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>n</th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>spearman_r</th>\n",
       "      <th>spearman_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT_41025 vs WVHT_41001</td>\n",
       "      <td>26436</td>\n",
       "      <td>0.731690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPD_41025 vs DPD_41001</td>\n",
       "      <td>26436</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APD_41025 vs APD_41001</td>\n",
       "      <td>26436</td>\n",
       "      <td>0.718417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.639295</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD_41025 vs WSPD_41001</td>\n",
       "      <td>186793</td>\n",
       "      <td>0.616417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592349</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pair       n  pearson_r  pearson_p  spearman_r  \\\n",
       "0  WVHT_41025 vs WVHT_41001   26436   0.731690        0.0    0.725564   \n",
       "1    DPD_41025 vs DPD_41001   26436   0.576803        0.0    0.519925   \n",
       "2    APD_41025 vs APD_41001   26436   0.718417        0.0    0.639295   \n",
       "3  WSPD_41025 vs WSPD_41001  186793   0.616417        0.0    0.592349   \n",
       "\n",
       "   spearman_p  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Align on common timestamps\n",
    "common = df_41025.join(\n",
    "    df_41001,\n",
    "    how=\"inner\",\n",
    "    lsuffix=\"_41025\",\n",
    "    rsuffix=\"_41001\"\n",
    ")\n",
    "\n",
    "print(\"Aligned 2019–2024 dataset (common timestamps only):\")\n",
    "print(\"  shape     :\", common.shape)\n",
    "print(\"  date range:\", common.index.min(), \"→\", common.index.max(), \"\\n\")\n",
    "\n",
    "pairs_to_check = [\n",
    "    (\"WVHT_41025\", \"WVHT_41001\"),\n",
    "    (\"DPD_41025\",  \"DPD_41001\"),\n",
    "    (\"APD_41025\",  \"APD_41001\"),\n",
    "    (\"WSPD_41025\", \"WSPD_41001\"),\n",
    "]\n",
    "\n",
    "print(\"Correlation between 41025 and 41001 (2019–2024, overlapping valid data):\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for col_41025, col_41001 in pairs_to_check:\n",
    "    print(f\"Checking pair: {col_41025} vs {col_41001}\")\n",
    "\n",
    "    if col_41025 not in common.columns or col_41001 not in common.columns:\n",
    "        print(\"  -> One or both columns not present, skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    sub = common[[col_41025, col_41001]].dropna()\n",
    "    print(f\"  Rows with both non-missing: {len(sub)}\")\n",
    "\n",
    "    if len(sub) < 100:\n",
    "        print(\"  -> Too few points (<100), skipping correlation.\\n\")\n",
    "        continue\n",
    "\n",
    "    r_p, p_p = pearsonr(sub[col_41025], sub[col_41001])\n",
    "    r_s, p_s = spearmanr(sub[col_41025], sub[col_41001])\n",
    "\n",
    "    results.append({\n",
    "        \"pair\": f\"{col_41025} vs {col_41001}\",\n",
    "        \"n\": len(sub),\n",
    "        \"pearson_r\": r_p,\n",
    "        \"pearson_p\": p_p,\n",
    "        \"spearman_r\": r_s,\n",
    "        \"spearman_p\": p_s,\n",
    "    })\n",
    "\n",
    "    print(f\"  Pearson  r = {r_p:.3f}, p = {p_p:.1e}\")\n",
    "    print(f\"  Spearman r = {r_s:.3f}, p = {p_s:.1e}\\n\")\n",
    "\n",
    "corr_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary table:\")\n",
    "display(corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc19c93d-f3ff-4093-a446-4974b0955c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WVHT missing at 41025 (2019–2024): 232253\n",
      "Missing at 41025 but available at 41001: 17493\n"
     ]
    }
   ],
   "source": [
    "# Focus on 2019–2024 WVHT as example\n",
    "mask_41025_missing = df_41025[\"WVHT\"].isna()\n",
    "mask_41001_avail   = df_41001[\"WVHT\"].notna()\n",
    "\n",
    "# Align neighbour mask to 41025 index\n",
    "mask_41001_avail_aligned = mask_41001_avail.reindex(df_41025.index, fill_value=False)\n",
    "\n",
    "overlap_mask = mask_41025_missing & mask_41001_avail_aligned\n",
    "\n",
    "print(\"Total WVHT missing at 41025 (2019–2024):\", mask_41025_missing.sum())\n",
    "print(\"Missing at 41025 but available at 41001:\", overlap_mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ea0b86-aea2-4cee-adf9-426c8c4d71a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1025.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:10:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1025.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:20:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1025.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:30:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1025.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:40:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.69</td>\n",
       "      <td>6.06</td>\n",
       "      <td>301.0</td>\n",
       "      <td>1025.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     WDIR  WSPD  GST  WVHT   DPD   APD    MWD    PRES  ATMP  \\\n",
       "datetime                                                                      \n",
       "2019-06-01 00:00:00   NaN   1.8  2.7   NaN   NaN   NaN    NaN  1025.6   NaN   \n",
       "2019-06-01 00:10:00   NaN   1.7  2.5   NaN   NaN   NaN    NaN  1025.6   NaN   \n",
       "2019-06-01 00:20:00   NaN   1.7  2.6   NaN   NaN   NaN    NaN  1025.7   NaN   \n",
       "2019-06-01 00:30:00   NaN   1.2  2.0   NaN   NaN   NaN    NaN  1025.6   NaN   \n",
       "2019-06-01 00:40:00   NaN   1.4  2.0  0.95  7.69  6.06  301.0  1025.6   NaN   \n",
       "\n",
       "                     WTMP  DEWP  VIS  TIDE  \n",
       "datetime                                    \n",
       "2019-06-01 00:00:00  22.5   NaN  NaN   NaN  \n",
       "2019-06-01 00:10:00  22.5   NaN  NaN   NaN  \n",
       "2019-06-01 00:20:00  22.5   NaN  NaN   NaN  \n",
       "2019-06-01 00:30:00  22.5   NaN  NaN   NaN  \n",
       "2019-06-01 00:40:00  22.5   NaN  NaN   NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_41025.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b795554e-fc76-4c51-8fff-d9657859aea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-01 18:40:00</th>\n",
       "      <td>76.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 18:50:00</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1028.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 19:00:00</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 19:10:00</th>\n",
       "      <td>69.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1028.9</td>\n",
       "      <td>25.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 19:20:00</th>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1028.7</td>\n",
       "      <td>25.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     WDIR  WSPD  GST  WVHT  DPD  APD  MWD    PRES  ATMP  WTMP  \\\n",
       "datetime                                                                        \n",
       "2019-05-01 18:40:00  76.0   0.1  1.2   NaN  NaN  NaN  NaN  1029.0  25.8   NaN   \n",
       "2019-05-01 18:50:00  57.0   0.1  0.8   NaN  NaN  NaN  NaN  1028.9  25.7   NaN   \n",
       "2019-05-01 19:00:00  65.0   0.1  1.2   NaN  NaN  NaN  NaN  1029.0  25.5   NaN   \n",
       "2019-05-01 19:10:00  69.0   0.1  0.7   NaN  NaN  NaN  NaN  1028.9  25.8   NaN   \n",
       "2019-05-01 19:20:00  64.0   0.1  1.3   NaN  NaN  NaN  NaN  1028.7  25.9   NaN   \n",
       "\n",
       "                     DEWP  VIS  TIDE  \n",
       "datetime                              \n",
       "2019-05-01 18:40:00  18.1  NaN   NaN  \n",
       "2019-05-01 18:50:00  18.1  NaN   NaN  \n",
       "2019-05-01 19:00:00  18.2  NaN   NaN  \n",
       "2019-05-01 19:10:00  18.3  NaN   NaN  \n",
       "2019-05-01 19:20:00  18.3  NaN   NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_41001.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67a1be9a-e859-4c2c-95fb-abe7a06cdefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Imputation model for WVHT ===\n",
      "  Training samples: 26436\n",
      "  MAE: 0.321\n",
      "  R² : 0.535\n",
      "  Approx: WVHT_41025 ≈ a * WVHT_41001 + b\n",
      "    a = 0.558, b = 0.424\n",
      "\n",
      "=== Imputation model for APD ===\n",
      "  Training samples: 26436\n",
      "  MAE: 0.600\n",
      "  R² : 0.516\n",
      "  Approx: APD_41025 ≈ a * APD_41001 + b\n",
      "    a = 0.741, b = 1.085\n",
      "\n",
      "=== Imputation model for DPD ===\n",
      "  Training samples: 26436\n",
      "  MAE: 1.494\n",
      "  R² : 0.333\n",
      "  Approx: DPD_41025 ≈ a * DPD_41001 + b\n",
      "    a = 0.639, b = 2.904\n",
      "\n",
      "=== Imputation model for WSPD ===\n",
      "  Training samples: 186793\n",
      "  MAE: 2.067\n",
      "  R² : 0.380\n",
      "  Approx: WSPD_41025 ≈ a * WSPD_41001 + b\n",
      "    a = 0.656, b = 2.683\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN REGRESSION MODELS (41001 → 41025) FOR KEY VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "vars_to_impute = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "imputation_models = {}\n",
    "\n",
    "for var in vars_to_impute:\n",
    "    col_t = f\"{var}_41025\"\n",
    "    col_n = f\"{var}_41001\"\n",
    "\n",
    "    if col_t not in common.columns or col_n not in common.columns:\n",
    "        print(f\"\\n Skipping {var}: {col_t} or {col_n} not found in 'common'.\")\n",
    "        continue\n",
    "\n",
    "    sub = common[[col_t, col_n]].dropna()\n",
    "    X = sub[[col_n]].values\n",
    "    y = sub[col_t].values\n",
    "\n",
    "    if len(sub) < 100:\n",
    "        print(f\"\\n Skipping {var}: too few overlapping points ({len(sub)}).\")\n",
    "        continue\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2  = r2_score(y, y_pred)\n",
    "\n",
    "    imputation_models[var] = {\n",
    "        \"model\": model,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"n_train\": len(sub)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Imputation model for {var} ===\")\n",
    "    print(f\"  Training samples: {len(sub)}\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  R² : {r2:.3f}\")\n",
    "    print(f\"  Approx: {var}_41025 ≈ a * {var}_41001 + b\")\n",
    "    print(f\"    a = {model.coef_[0]:.3f}, b = {model.intercept_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec292f1-6eb8-4dfa-9755-ecbe7f237ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WVHT ===\n",
      "  Total missing at 41025: 232253\n",
      "  Can impute with 41001:  17493\n",
      "\n",
      "=== APD ===\n",
      "  Total missing at 41025: 232253\n",
      "  Can impute with 41001:  17493\n",
      "\n",
      "=== DPD ===\n",
      "  Total missing at 41025: 232385\n",
      "  Can impute with 41001:  17493\n",
      "\n",
      "=== WSPD ===\n",
      "  Total missing at 41025: 1333\n",
      "  Can impute with 41001:  238\n",
      "\n",
      "Imputation summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>total_missing</th>\n",
       "      <th>imputed</th>\n",
       "      <th>still_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT</td>\n",
       "      <td>232253</td>\n",
       "      <td>17493</td>\n",
       "      <td>214760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APD</td>\n",
       "      <td>232253</td>\n",
       "      <td>17493</td>\n",
       "      <td>214760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPD</td>\n",
       "      <td>232385</td>\n",
       "      <td>17493</td>\n",
       "      <td>214892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD</td>\n",
       "      <td>1333</td>\n",
       "      <td>238</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable  total_missing  imputed  still_missing\n",
       "0     WVHT         232253    17493         214760\n",
       "1      APD         232253    17493         214760\n",
       "2      DPD         232385    17493         214892\n",
       "3     WSPD           1333      238           1095"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPUTATION LOOP\n",
    "\n",
    "df_41025_imputed = df_41025.copy()\n",
    "imputation_log = []\n",
    "\n",
    "for var in vars_to_impute:\n",
    "    if var not in imputation_models:\n",
    "        continue\n",
    "\n",
    "    model = imputation_models[var][\"model\"]\n",
    "\n",
    "    # Masks defined on their own DataFrames\n",
    "    missing_25 = df_41025[var].isna()\n",
    "    avail_01   = df_41001[var].notna()\n",
    "\n",
    "    # Align neighbour mask to 41025 index\n",
    "    avail_01_aligned = avail_01.reindex(df_41025.index, fill_value=False)\n",
    "\n",
    "    # Only positions where 41025 is missing AND 41001 has data\n",
    "    to_impute_mask = missing_25 & avail_01_aligned\n",
    "\n",
    "    n_total_missing = int(missing_25.sum())\n",
    "    n_can_impute    = int(to_impute_mask.sum())\n",
    "\n",
    "    print(f\"\\n=== {var} ===\")\n",
    "    print(f\"  Total missing at 41025: {n_total_missing}\")\n",
    "    print(f\"  Can impute with 41001:  {n_can_impute}\")\n",
    "\n",
    "    if n_can_impute == 0:\n",
    "        continue\n",
    "\n",
    "    # Get timestamps to impute\n",
    "    idx_to_impute = df_41025.index[to_impute_mask]\n",
    "\n",
    "    # Neighbour values at those timestamps (using same index)\n",
    "    neigh_vals = df_41001.loc[idx_to_impute, var].values.reshape(-1, 1)\n",
    "\n",
    "    # Predict\n",
    "    imputed_vals = model.predict(neigh_vals)\n",
    "\n",
    "    # Assign back using the index list, not the boolean series\n",
    "    df_41025_imputed.loc[idx_to_impute, var] = imputed_vals\n",
    "\n",
    "    # Flag column\n",
    "    flag_col = f\"{var}_imputed_flag\"\n",
    "    if flag_col not in df_41025_imputed.columns:\n",
    "        df_41025_imputed[flag_col] = False\n",
    "    df_41025_imputed.loc[idx_to_impute, flag_col] = True\n",
    "\n",
    "    imputation_log.append({\n",
    "        \"variable\": var,\n",
    "        \"total_missing\": n_total_missing,\n",
    "        \"imputed\": n_can_impute,\n",
    "        \"still_missing\": n_total_missing - n_can_impute\n",
    "    })\n",
    "\n",
    "imputation_log = pd.DataFrame(imputation_log)\n",
    "print(\"\\nImputation summary:\")\n",
    "display(imputation_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dbd3c38-88bd-4114-b565-857ad2bd681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing % AFTER imputation:\n",
      "WVHT    77.147455\n",
      "APD     77.147455\n",
      "DPD     77.194873\n",
      "WSPD     0.393353\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing % AFTER imputation:\")\n",
    "print(df_41025_imputed[vars_to_impute].isna().mean().mul(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb24b2-a650-4a4b-b94b-55d36d5e0045",
   "metadata": {},
   "source": [
    "# Final Decision on Study Period and Neighbour Use\n",
    "\n",
    "##  Why 2019–2024 Was Rejected\n",
    "\n",
    "We initially tried to use the 2019–2024 period for both the target station (41025 – Diamond Shoals) and the neighbour station (41001 – East Hatteras) so that we could perform spatial imputation with full temporal overlap.\n",
    "\n",
    "However, the data quality analysis for 2019–2024 at 41025 showed:\n",
    "\n",
    "- Extremely high missingness for key wave variables:\n",
    "  - Significant wave height (WVHT)\n",
    "  - Average wave period (APD)\n",
    "  - Dominant wave period (DPD)\n",
    "  - Wind speed (WSPD)\n",
    "\n",
    "Even after applying neighbour-based imputation using 41001, only a small fraction of these missing values could be filled:\n",
    "\n",
    "- About 17,500 timestamps were imputed, but over 230,000 remained missing for WVHT/APD/DPD.\n",
    "- Post-imputation missingness for these variables remained around 77–80%.\n",
    "\n",
    "This means that, for 2019–2024:\n",
    "\n",
    "- The 41025 record is **too sparse** to support robust time-series modeling.\n",
    "- Any machine learning model trained on this period would be dominated by missing data and imputation artifacts.\n",
    "\n",
    "From a statistical and scientific standpoint, this makes 2019–2024 unsuitable as the primary study period for forecasting at Diamond Shoals.\n",
    "\n",
    "\n",
    "## Returning to 2014–2019 for 41025\n",
    "\n",
    "The original paper used **2014–2019** for station 41025. Our new analysis confirms that:\n",
    "\n",
    "- 2014–2019 at 41025 has substantially **better coverage** of WVHT, APD, DPD, and WSPD.\n",
    "- This period provides enough continuous data to:\n",
    "  - Build lagged features,\n",
    "  - Train and validate time-series models,\n",
    "  - Evaluate multi-horizon forecasts in a statistically meaningful way.\n",
    "\n",
    "Therefore, we explicitly **adopt 2014–2019 at 41025 as the main study period** for this project.\n",
    "\n",
    "The research focus remains:\n",
    "\n",
    "> Accurate wave energy forecasting at Diamond Shoals (Station 41025),  \n",
    "> with improved methodology over the original paper (validation, feature engineering, model comparison).\n",
    "\n",
    "\n",
    "## Role of Neighbour Station 41001\n",
    "\n",
    "Station 41001 (East Hatteras) remains valuable but plays a **supporting role**:\n",
    "\n",
    "- 41001 will be used **only for optional imputation** in years where:\n",
    "  - 41025 has gaps, and\n",
    "  - 41001 has overlapping observations for the same timestamps.\n",
    "- We will not force 41001 into every year or every gap.\n",
    "- We will not use 41001 directly as a predictor in the forecasting models for 41025; its role is purely to help clean up missing values where justified.\n",
    "\n",
    "If 41001 does not overlap sufficiently with 41025 in some years (e.g., early 2014), we will:\n",
    "\n",
    "- Leave those gaps as they are, or\n",
    "- Use simple, local methods (short-range interpolation or forward fill/backward fill) for small gaps,\n",
    "- But we will **not** over-impute using distant or weakly related data.\n",
    "\n",
    "This ensures that the modeling remains grounded in actual 41025 measurements.\n",
    "\n",
    "## Final Study Setup Going Forward\n",
    "\n",
    "From this point onward in the notebook, we adopt the following setup:\n",
    "\n",
    "- **Target station:** 41025 (Diamond Shoals, NC)\n",
    "- **Primary study period:** 2014-01-01 to 2019-12-31\n",
    "- **Neighbour station:** 41001 (East Hatteras), used only opportunistically for imputation where:\n",
    "  - timestamps overlap with 41025, and\n",
    "  - correlations are strong (especially for WVHT, APD, DPD, WSPD).\n",
    "\n",
    "All subsequent steps (gap analysis, imputation, feature engineering, train/validation/test splits, and model development) will be based on this **2014–2019 target dataset**.\n",
    "\n",
    "We keep the earlier 2019–2024 experiments in the notebook for transparency, but they are no longer part of the main analysis pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab551764-7adc-4242-9299-9232b554a8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:\n",
      "   41025h2014.txt\n",
      "   41025h2015.txt\n",
      "   41025h2016.txt\n",
      "   41025h2017.txt\n",
      "   41025h2018.txt\n",
      "   41025h2019.txt\n",
      "   41025h2020.txt\n",
      "   41025h2021.txt\n",
      "   41025h2022.txt\n",
      "   41025h2023.txt\n",
      "   41025h2024.txt\n",
      "Found files:\n",
      "   41001h2015.txt\n",
      "   41001h2016.txt\n",
      "   41001h2017.txt\n",
      "   41001h2019.txt\n",
      "   41001h2020.txt\n",
      "   41001h2021.txt\n",
      "   41001h2022.txt\n",
      "   41001h2023.txt\n",
      "   41001h2024.txt\n",
      "41025 2014–2019: 2014-01-01 00:50:00 → 2019-12-31 23:50:00 117881\n",
      "41001 2014–2019: 2015-07-22 06:30:00 → 2019-11-15 19:00:00 85330\n",
      "\n",
      "Missing % at 41025 (2014–2019) for key variables:\n",
      "WVHT    62.441785\n",
      "APD     62.441785\n",
      "DPD     62.441785\n",
      "WSPD     0.921268\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL DATA SELECTION: 2014–2019 FOR STATION 41025\n",
    "# ============================================================================\n",
    "\n",
    "# Rebuild full dataframes if needed\n",
    "df_41025_full = build_station_df(\"41025h\")\n",
    "df_41001_full = build_station_df(\"41001h\")\n",
    "\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE   = \"2019-12-31\"\n",
    "\n",
    "df_41025 = df_41025_full.loc[START_DATE:END_DATE].copy()\n",
    "df_41001 = df_41001_full.loc[START_DATE:END_DATE].copy()\n",
    "\n",
    "print(\"41025 2014–2019:\", df_41025.index.min(), \"→\", df_41025.index.max(), len(df_41025))\n",
    "print(\"41001 2014–2019:\", df_41001.index.min(), \"→\", df_41001.index.max(), len(df_41001))\n",
    "\n",
    "print(\"\\nMissing % at 41025 (2014–2019) for key variables:\")\n",
    "print(df_41025[[\"WVHT\",\"APD\",\"DPD\",\"WSPD\"]].isna().mean().mul(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22ce1b7e-efb6-465e-99a6-9f46276e63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned 2014–2019 dataset (common timestamps only):\n",
      "  shape     : (32989, 26)\n",
      "  date range: 2015-07-29 18:50:00 → 2019-11-15 19:00:00 \n",
      "\n",
      "Correlation between 41025 and 41001 (2014–2019, overlapping valid data):\n",
      "\n",
      "Checking pair: WVHT_41025 vs WVHT_41001\n",
      "  Rows with both non-missing: 3941\n",
      "  Pearson  r = 0.785, p = 0.0e+00\n",
      "  Spearman r = 0.750, p = 0.0e+00\n",
      "\n",
      "Checking pair: APD_41025 vs APD_41001\n",
      "  Rows with both non-missing: 3941\n",
      "  Pearson  r = 0.650, p = 0.0e+00\n",
      "  Spearman r = 0.591, p = 0.0e+00\n",
      "\n",
      "Checking pair: DPD_41025 vs DPD_41001\n",
      "  Rows with both non-missing: 3941\n",
      "  Pearson  r = 0.517, p = 1.0e-268\n",
      "  Spearman r = 0.507, p = 6.5e-257\n",
      "\n",
      "Checking pair: WSPD_41025 vs WSPD_41001\n",
      "  Rows with both non-missing: 32962\n",
      "  Pearson  r = 0.647, p = 0.0e+00\n",
      "  Spearman r = 0.614, p = 0.0e+00\n",
      "\n",
      "\n",
      "Summary table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>n</th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>spearman_r</th>\n",
       "      <th>spearman_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT_41025 vs WVHT_41001</td>\n",
       "      <td>3941</td>\n",
       "      <td>0.785452</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.749941</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APD_41025 vs APD_41001</td>\n",
       "      <td>3941</td>\n",
       "      <td>0.649992</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.590726</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPD_41025 vs DPD_41001</td>\n",
       "      <td>3941</td>\n",
       "      <td>0.517310</td>\n",
       "      <td>1.003016e-268</td>\n",
       "      <td>0.507377</td>\n",
       "      <td>6.526329e-257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD_41025 vs WSPD_41001</td>\n",
       "      <td>32962</td>\n",
       "      <td>0.647217</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.613756</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pair      n  pearson_r      pearson_p  spearman_r  \\\n",
       "0  WVHT_41025 vs WVHT_41001   3941   0.785452   0.000000e+00    0.749941   \n",
       "1    APD_41025 vs APD_41001   3941   0.649992   0.000000e+00    0.590726   \n",
       "2    DPD_41025 vs DPD_41001   3941   0.517310  1.003016e-268    0.507377   \n",
       "3  WSPD_41025 vs WSPD_41001  32962   0.647217   0.000000e+00    0.613756   \n",
       "\n",
       "      spearman_p  \n",
       "0   0.000000e+00  \n",
       "1   0.000000e+00  \n",
       "2  6.526329e-257  \n",
       "3   0.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Align on common timestamps (2014–2019)\n",
    "common_1419 = df_41025.join(\n",
    "    df_41001,\n",
    "    how=\"inner\",\n",
    "    lsuffix=\"_41025\",\n",
    "    rsuffix=\"_41001\"\n",
    ")\n",
    "\n",
    "print(\"Aligned 2014–2019 dataset (common timestamps only):\")\n",
    "print(\"  shape     :\", common_1419.shape)\n",
    "print(\"  date range:\", common_1419.index.min(), \"→\", common_1419.index.max(), \"\\n\")\n",
    "\n",
    "pairs_to_check = [\n",
    "    (\"WVHT_41025\", \"WVHT_41001\"),\n",
    "    (\"APD_41025\",  \"APD_41001\"),\n",
    "    (\"DPD_41025\",  \"DPD_41001\"),\n",
    "    (\"WSPD_41025\", \"WSPD_41001\"),\n",
    "]\n",
    "\n",
    "print(\"Correlation between 41025 and 41001 (2014–2019, overlapping valid data):\\n\")\n",
    "\n",
    "results = []\n",
    "for col_25, col_01 in pairs_to_check:\n",
    "    print(f\"Checking pair: {col_25} vs {col_01}\")\n",
    "    if col_25 not in common_1419.columns or col_01 not in common_1419.columns:\n",
    "        print(\"  -> Columns not present, skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    sub = common_1419[[col_25, col_01]].dropna()\n",
    "    print(f\"  Rows with both non-missing: {len(sub)}\")\n",
    "    if len(sub) < 100:\n",
    "        print(\"  -> Too few points, skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    r_p, p_p = pearsonr(sub[col_25], sub[col_01])\n",
    "    r_s, p_s = spearmanr(sub[col_25], sub[col_01])\n",
    "\n",
    "    results.append({\n",
    "        \"pair\": f\"{col_25} vs {col_01}\",\n",
    "        \"n\": len(sub),\n",
    "        \"pearson_r\": r_p,\n",
    "        \"pearson_p\": p_p,\n",
    "        \"spearman_r\": r_s,\n",
    "        \"spearman_p\": p_s,\n",
    "    })\n",
    "\n",
    "    print(f\"  Pearson  r = {r_p:.3f}, p = {p_p:.1e}\")\n",
    "    print(f\"  Spearman r = {r_s:.3f}, p = {p_s:.1e}\\n\")\n",
    "\n",
    "corr_1419 = pd.DataFrame(results)\n",
    "print(\"\\nSummary table:\")\n",
    "display(corr_1419)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1af0c28-2c95-4509-b291-1ae8b7214a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WVHT ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41001: 13\n",
      "\n",
      "=== APD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41001: 13\n",
      "\n",
      "=== DPD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41001: 13\n",
      "\n",
      "=== WSPD ===\n",
      "  Total missing at 41025: 1086\n",
      "  Missing at 41025 but available at 41001: 13\n"
     ]
    }
   ],
   "source": [
    "vars_to_impute = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "for var in vars_to_impute:\n",
    "    print(f\"\\n=== {var} ===\")\n",
    "    missing_25 = df_41025[var].isna()\n",
    "    avail_01   = df_41001[var].notna()\n",
    "    avail_01_aligned = avail_01.reindex(df_41025.index, fill_value=False)\n",
    "\n",
    "    total_missing = missing_25.sum()\n",
    "    can_impute    = (missing_25 & avail_01_aligned).sum()\n",
    "\n",
    "    print(f\"  Total missing at 41025: {total_missing}\")\n",
    "    print(f\"  Missing at 41025 but available at 41001: {can_impute}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5fadb-0fec-4425-8d64-11c2dffa33ef",
   "metadata": {},
   "source": [
    "## Load 41002 and align with 41025 (2014–2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea471bc7-ad02-41c2-8689-bf4d00df9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:\n",
      "   41002h2014.txt\n",
      "   41002h2015.txt\n",
      "   41002h2016.txt\n",
      "   41002h2017.txt\n",
      "   41002h2018.txt\n",
      "   41002h2019.txt\n",
      "41025 2014–2019: 2014-01-01 00:50:00 → 2019-12-31 23:50:00 117881\n",
      "41002 2014–2019: 2014-01-01 00:50:00 → 2019-12-31 23:50:00 123491\n",
      "\n",
      "Aligned 2014–2019 dataset (common timestamps only):\n",
      "  shape     : (97892, 26)\n",
      "  date range: 2014-01-01 00:50:00 → 2019-12-31 23:50:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD NEIGHBOUR STATION 41002 AND ALIGN WITH 41025 (2014–2019)\n",
    "# ============================================================================\n",
    "\n",
    "# Build full dataframe for 41002 from 2014–2019 files\n",
    "df_41002_full = build_station_df(\"41002h\")\n",
    "\n",
    "# Restrict to the same 2014–2019 window\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE   = \"2019-12-31\"\n",
    "\n",
    "df_41025 = df_41025_full.loc[START_DATE:END_DATE].copy()\n",
    "df_41002 = df_41002_full.loc[START_DATE:END_DATE].copy()\n",
    "\n",
    "print(\"41025 2014–2019:\", df_41025.index.min(), \"→\", df_41025.index.max(), len(df_41025))\n",
    "print(\"41002 2014–2019:\", df_41002.index.min(), \"→\", df_41002.index.max(), len(df_41002))\n",
    "\n",
    "# Align on common timestamps\n",
    "common_2502 = df_41025.join(\n",
    "    df_41002,\n",
    "    how=\"inner\",\n",
    "    lsuffix=\"_41025\",\n",
    "    rsuffix=\"_41002\"\n",
    ")\n",
    "\n",
    "print(\"\\nAligned 2014–2019 dataset (common timestamps only):\")\n",
    "print(\"  shape     :\", common_2502.shape)\n",
    "print(\"  date range:\", common_2502.index.min(), \"→\", common_2502.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f8bbfbf-b30f-4fd9-9d6b-f1f2e37c6d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation between 41025 and 41002 (2014–2019):\n",
      "\n",
      "=== WVHT_41025 vs WVHT_41002 ===\n",
      "  Rows with both non-missing: 36073\n",
      "  Pearson  r = 0.712, p = 0.0e+00\n",
      "  Spearman r = 0.677, p = 0.0e+00\n",
      "\n",
      "=== APD_41025 vs APD_41002 ===\n",
      "  Rows with both non-missing: 36073\n",
      "  Pearson  r = 0.586, p = 0.0e+00\n",
      "  Spearman r = 0.501, p = 0.0e+00\n",
      "\n",
      "=== DPD_41025 vs DPD_41002 ===\n",
      "  Rows with both non-missing: 36073\n",
      "  Pearson  r = 0.492, p = 0.0e+00\n",
      "  Spearman r = 0.429, p = 0.0e+00\n",
      "\n",
      "=== WSPD_41025 vs WSPD_41002 ===\n",
      "  Rows with both non-missing: 95171\n",
      "  Pearson  r = 0.529, p = 0.0e+00\n",
      "  Spearman r = 0.489, p = 0.0e+00\n",
      "\n",
      "Summary table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>n</th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>spearman_r</th>\n",
       "      <th>spearman_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT</td>\n",
       "      <td>36073</td>\n",
       "      <td>0.712471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APD</td>\n",
       "      <td>36073</td>\n",
       "      <td>0.586065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500713</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPD</td>\n",
       "      <td>36073</td>\n",
       "      <td>0.491563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD</td>\n",
       "      <td>95171</td>\n",
       "      <td>0.528794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489422</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var      n  pearson_r  pearson_p  spearman_r  spearman_p\n",
       "0  WVHT  36073   0.712471        0.0    0.676639         0.0\n",
       "1   APD  36073   0.586065        0.0    0.500713         0.0\n",
       "2   DPD  36073   0.491563        0.0    0.428582         0.0\n",
       "3  WSPD  95171   0.528794        0.0    0.489422         0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputable gaps (41025 missing, 41002 available):\n",
      "\n",
      "=== WVHT ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41002: 2103\n",
      "\n",
      "=== APD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41002: 2103\n",
      "\n",
      "=== DPD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Missing at 41025 but available at 41002: 2103\n",
      "\n",
      "=== WSPD ===\n",
      "  Total missing at 41025: 1086\n",
      "  Missing at 41025 but available at 41002: 1036\n",
      "\n",
      "Imputable summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>total_missing</th>\n",
       "      <th>imputable_with_41002</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APD</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPD</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD</td>\n",
       "      <td>1086</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var  total_missing  imputable_with_41002\n",
       "0  WVHT          73607                  2103\n",
       "1   APD          73607                  2103\n",
       "2   DPD          73607                  2103\n",
       "3  WSPD           1086                  1036"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "vars_to_impute = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Correlations\n",
    "# -------------------------------------------------------\n",
    "print(\"\\nCorrelation between 41025 and 41002 (2014–2019):\\n\")\n",
    "\n",
    "corr_rows = []\n",
    "for var in vars_to_impute:\n",
    "    col_t = f\"{var}_41025\"\n",
    "    col_n = f\"{var}_41002\"\n",
    "\n",
    "    print(f\"=== {col_t} vs {col_n} ===\")\n",
    "\n",
    "    if col_t not in common_2502.columns or col_n not in common_2502.columns:\n",
    "        print(\"  -> Columns not present, skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    sub = common_2502[[col_t, col_n]].dropna()\n",
    "    n = len(sub)\n",
    "    print(f\"  Rows with both non-missing: {n}\")\n",
    "\n",
    "    if n < 100:\n",
    "        print(\"  -> Too few points (<100), skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    r_p, p_p = pearsonr(sub[col_t], sub[col_n])\n",
    "    r_s, p_s = spearmanr(sub[col_t], sub[col_n])\n",
    "\n",
    "    print(f\"  Pearson  r = {r_p:.3f}, p = {p_p:.1e}\")\n",
    "    print(f\"  Spearman r = {r_s:.3f}, p = {p_s:.1e}\\n\")\n",
    "\n",
    "    corr_rows.append({\n",
    "        \"var\": var,\n",
    "        \"n\": n,\n",
    "        \"pearson_r\": r_p,\n",
    "        \"pearson_p\": p_p,\n",
    "        \"spearman_r\": r_s,\n",
    "        \"spearman_p\": p_s,\n",
    "    })\n",
    "\n",
    "corr_2502 = pd.DataFrame(corr_rows)\n",
    "print(\"Summary table:\")\n",
    "display(corr_2502)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# How many missing at 41025 can be imputed from 41002?\n",
    "# -------------------------------------------------------\n",
    "print(\"\\nImputable gaps (41025 missing, 41002 available):\\n\")\n",
    "\n",
    "gap_rows = []\n",
    "for var in vars_to_impute:\n",
    "    print(f\"=== {var} ===\")\n",
    "\n",
    "    if var not in df_41025.columns or var not in df_41002.columns:\n",
    "        print(\"  -> Column not present at one of the stations.\\n\")\n",
    "        continue\n",
    "\n",
    "    missing_25 = df_41025[var].isna()\n",
    "    avail_02   = df_41002[var].notna()\n",
    "    avail_02_aligned = avail_02.reindex(df_41025.index, fill_value=False)\n",
    "\n",
    "    total_missing = int(missing_25.sum())\n",
    "    can_impute    = int((missing_25 & avail_02_aligned).sum())\n",
    "\n",
    "    print(f\"  Total missing at 41025: {total_missing}\")\n",
    "    print(f\"  Missing at 41025 but available at 41002: {can_impute}\\n\")\n",
    "\n",
    "    gap_rows.append({\n",
    "        \"var\": var,\n",
    "        \"total_missing\": total_missing,\n",
    "        \"imputable_with_41002\": can_impute\n",
    "    })\n",
    "\n",
    "gaps_2502 = pd.DataFrame(gap_rows)\n",
    "print(\"Imputable summary:\")\n",
    "display(gaps_2502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5f6174f-3ac2-43ea-b231-3001e1619fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Imputation model for WVHT (41002 → 41025) ===\n",
      "  Training samples: 36073\n",
      "  MAE: 0.380\n",
      "  R² : 0.508\n",
      "  Approx: WVHT_41025 ≈ a * WVHT_41002 + b\n",
      "    a = 0.567, b = 0.537\n",
      "\n",
      "=== Imputation model for APD (41002 → 41025) ===\n",
      "  Training samples: 36073\n",
      "  MAE: 0.676\n",
      "  R² : 0.343\n",
      "  Approx: APD_41025 ≈ a * APD_41002 + b\n",
      "    a = 0.605, b = 1.956\n",
      "\n",
      "=== Imputation model for DPD (41002 → 41025) ===\n",
      "  Training samples: 36073\n",
      "  MAE: 1.561\n",
      "  R² : 0.242\n",
      "  Approx: DPD_41025 ≈ a * DPD_41002 + b\n",
      "    a = 0.532, b = 3.691\n",
      "\n",
      "=== Imputation model for WSPD (41002 → 41025) ===\n",
      "  Training samples: 95171\n",
      "  MAE: 2.398\n",
      "  R² : 0.280\n",
      "  Approx: WSPD_41025 ≈ a * WSPD_41002 + b\n",
      "    a = 0.590, b = 3.504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "vars_to_impute = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "imputation_models_2502 = {}\n",
    "\n",
    "for var in vars_to_impute:\n",
    "    col_t = f\"{var}_41025\"\n",
    "    col_n = f\"{var}_41002\"\n",
    "\n",
    "    if col_t not in common_2502.columns or col_n not in common_2502.columns:\n",
    "        print(f\"\\n Skipping {var}: {col_t} or {col_n} not in common_2502.\")\n",
    "        continue\n",
    "\n",
    "    sub = common_2502[[col_t, col_n]].dropna()\n",
    "    X = sub[[col_n]].values\n",
    "    y = sub[col_t].values\n",
    "\n",
    "    if len(sub) < 100:\n",
    "        print(f\"\\n Skipping {var}: too few overlapping points ({len(sub)}).\")\n",
    "        continue\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2  = r2_score(y, y_pred)\n",
    "\n",
    "    imputation_models_2502[var] = {\n",
    "        \"model\": model,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"n_train\": len(sub)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Imputation model for {var} (41002 → 41025) ===\")\n",
    "    print(f\"  Training samples: {len(sub)}\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  R² : {r2:.3f}\")\n",
    "    print(f\"  Approx: {var}_41025 ≈ a * {var}_41002 + b\")\n",
    "    print(f\"    a = {model.coef_[0]:.3f}, b = {model.intercept_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3676772-5986-4611-a756-6bb5772b7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WVHT ===\n",
      "  Total missing at 41025: 73607\n",
      "  Can impute with 41002:  2103\n",
      "\n",
      "=== APD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Can impute with 41002:  2103\n",
      "\n",
      "=== DPD ===\n",
      "  Total missing at 41025: 73607\n",
      "  Can impute with 41002:  2103\n",
      "\n",
      "=== WSPD ===\n",
      "  Total missing at 41025: 1086\n",
      "  Can impute with 41002:  1036\n",
      "\n",
      "Spatial imputation summary (41002 → 41025):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>total_missing</th>\n",
       "      <th>imputed_with_41002</th>\n",
       "      <th>still_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WVHT</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "      <td>71504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APD</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "      <td>71504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPD</td>\n",
       "      <td>73607</td>\n",
       "      <td>2103</td>\n",
       "      <td>71504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSPD</td>\n",
       "      <td>1086</td>\n",
       "      <td>1036</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable  total_missing  imputed_with_41002  still_missing\n",
       "0     WVHT          73607                2103          71504\n",
       "1      APD          73607                2103          71504\n",
       "2      DPD          73607                2103          71504\n",
       "3     WSPD           1086                1036             50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY SPATIAL IMPUTATION: 41002 → 41025 (2014–2019)\n",
    "# ============================================================================\n",
    "\n",
    "df_41025_imputed = df_41025.copy()\n",
    "imputation_log_2502 = []\n",
    "\n",
    "for var in vars_to_impute:\n",
    "    if var not in imputation_models_2502:\n",
    "        continue\n",
    "\n",
    "    model = imputation_models_2502[var][\"model\"]\n",
    "\n",
    "    missing_25 = df_41025[var].isna()\n",
    "    avail_02   = df_41002[var].notna()\n",
    "    avail_02_aligned = avail_02.reindex(df_41025.index, fill_value=False)\n",
    "\n",
    "    to_impute_mask = missing_25 & avail_02_aligned\n",
    "?????????    n_total_missing = int(missing_25.sum())\n",
    "    n_can_impute    = int(to_impute_mask.sum())\n",
    "\n",
    "    print(f\"\\n=== {var} ===\")\n",
    "    print(f\"  Total missing at 41025: {n_total_missing}\")\n",
    "    print(f\"  Can impute with 41002:  {n_can_impute}\")\n",
    "\n",
    "    if n_can_impute == 0:\n",
    "        continue\n",
    "\n",
    "    idx_to_impute = df_41025.index[to_impute_mask]\n",
    "    neigh_vals = df_41002.loc[idx_to_impute, var].values.reshape(-1, 1)\n",
    "    imputed_vals = model.predict(neigh_vals)\n",
    "\n",
    "    df_41025_imputed.loc[idx_to_impute, var] = imputed_vals\n",
    "\n",
    "    flag_col = f\"{var}_imp_from_41002\"\n",
    "    if flag_col not in df_41025_imputed.columns:\n",
    "        df_41025_imputed[flag_col] = False\n",
    "    df_41025_imputed.loc[idx_to_impute, flag_col] = True\n",
    "\n",
    "    imputation_log_2502.append({\n",
    "        \"variable\": var,\n",
    "        \"total_missing\": n_total_missing,\n",
    "        \"imputed_with_41002\": n_can_impute,\n",
    "        \"still_missing\": n_total_missing - n_can_impute\n",
    "    })\n",
    "\n",
    "imputation_log_2502 = pd.DataFrame(imputation_log_2502)\n",
    "print(\"\\nSpatial imputation summary (41002 → 41025):\")\n",
    "display(imputation_log_2502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "699a442d-c4ee-4feb-b5d4-da9b10b38691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missingness before vs after spatial imputation (41002 → 41025):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_before_%</th>\n",
       "      <th>missing_after_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WVHT</th>\n",
       "      <td>62.441785</td>\n",
       "      <td>60.657782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APD</th>\n",
       "      <td>62.441785</td>\n",
       "      <td>60.657782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPD</th>\n",
       "      <td>62.441785</td>\n",
       "      <td>60.657782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSPD</th>\n",
       "      <td>0.921268</td>\n",
       "      <td>0.042416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      missing_before_%  missing_after_%\n",
       "WVHT         62.441785        60.657782\n",
       "APD          62.441785        60.657782\n",
       "DPD          62.441785        60.657782\n",
       "WSPD          0.921268         0.042416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "core_vars = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "missing_before = df_41025[core_vars].isna().mean().mul(100).rename(\"missing_before_%\")\n",
    "missing_after  = df_41025_imputed[core_vars].isna().mean().mul(100).rename(\"missing_after_%\")\n",
    "\n",
    "missing_report_2502 = pd.concat([missing_before, missing_after], axis=1)\n",
    "print(\"Missingness before vs after spatial imputation (41002 → 41025):\")\n",
    "display(missing_report_2502)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd609bf5-ae8e-4d28-bf46-1891736e8389",
   "metadata": {},
   "source": [
    "# Splitting Data and applying imputataion technique again to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a48d49f3-93b3-45d5-bdec-b3bb6379bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2014-01-01\"\n",
    "TRAIN_END  = \"2017-12-31 23:00:00\"\n",
    "VAL_END    = \"2018-12-31 23:00:00\"\n",
    "END_DATE   = \"2019-12-31 23:00:00\"\n",
    "\n",
    "df25 = df_41025_full.loc[START_DATE:END_DATE].copy()\n",
    "df02 = df_41002_full.loc[START_DATE:END_DATE].copy()\n",
    "\n",
    "# Date-based slices for each split\n",
    "df25_train = df25.loc[:TRAIN_END]\n",
    "df25_val   = df25.loc[TRAIN_END:VAL_END]\n",
    "df25_test  = df25.loc[VAL_END:]\n",
    "\n",
    "df02_train = df02.loc[:TRAIN_END]\n",
    "df02_val   = df02.loc[TRAIN_END:VAL_END]\n",
    "df02_test  = df02.loc[VAL_END:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04692f58-b94a-4351-b503-bd758228efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_train shape: (29915, 26)\n",
      "common_train date range: 2014-01-01 00:50:00 → 2017-12-31 22:50:00\n"
     ]
    }
   ],
   "source": [
    "vars_spatial = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "# Align on timestamps within train\n",
    "common_train = df25_train.join(\n",
    "    df02_train,\n",
    "    how=\"inner\",\n",
    "    lsuffix=\"_41025\",\n",
    "    rsuffix=\"_41002\"\n",
    ")\n",
    "\n",
    "print(\"common_train shape:\", common_train.shape)\n",
    "print(\"common_train date range:\", common_train.index.min(), \"→\", common_train.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b81483e-2455-409c-86ab-d365e3cef296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WVHT (train only) ===\n",
      "  n_train: 28521, MAE: 0.378, R²: 0.496\n",
      "\n",
      "=== APD (train only) ===\n",
      "  n_train: 28521, MAE: 0.676, R²: 0.336\n",
      "\n",
      "=== DPD (train only) ===\n",
      "  n_train: 28521, MAE: 1.572, R²: 0.236\n",
      "\n",
      "=== WSPD (train only) ===\n",
      "  n_train: 28544, MAE: 2.512, R²: 0.212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "spatial_models = {}\n",
    "\n",
    "for var in vars_spatial:\n",
    "    col_t = f\"{var}_41025\"\n",
    "    col_n = f\"{var}_41002\"\n",
    "\n",
    "    if col_t not in common_train.columns or col_n not in common_train.columns:\n",
    "        continue\n",
    "\n",
    "    sub = common_train[[col_t, col_n]].dropna()\n",
    "    if len(sub) < 100:\n",
    "        continue\n",
    "\n",
    "    X = sub[[col_n]].values\n",
    "    y = sub[col_t].values\n",
    "\n",
    "    m = LinearRegression().fit(X, y)\n",
    "    y_pred = m.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2  = r2_score(y, y_pred)\n",
    "\n",
    "    spatial_models[var] = {\"model\": m, \"mae\": mae, \"r2\": r2, \"n_train\": len(sub)}\n",
    "\n",
    "    print(f\"\\n=== {var} (train only) ===\")\n",
    "    print(f\"  n_train: {len(sub)}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0146869b-73e1-4afc-9bb5-6e67825be2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spatial(df_target, df_neigh, spatial_models, vars_spatial):\n",
    "    df_imp = df_target.copy()\n",
    "    logs = []\n",
    "\n",
    "    for var in vars_spatial:\n",
    "        if var not in spatial_models:\n",
    "            continue\n",
    "\n",
    "        m = spatial_models[var][\"model\"]\n",
    "\n",
    "        # Masks defined on the SAME index (df_target)\n",
    "        missing = df_imp[var].isna()\n",
    "        avail   = df_neigh[var].reindex(df_imp.index).notna()  # align neighbour index\n",
    "        to_imp  = missing & avail\n",
    "\n",
    "        n_total = int(missing.sum())\n",
    "        n_imp   = int(to_imp.sum())\n",
    "\n",
    "        print(f\"\\n=== {var} ===\")\n",
    "        print(f\"  total_missing: {n_total}, imputable with neighbour: {n_imp}\")\n",
    "\n",
    "        if n_imp == 0:\n",
    "            logs.append({\"var\": var, \"total_missing\": n_total,\n",
    "                         \"imputed_spatial\": 0, \"still_missing\": n_total})\n",
    "            continue\n",
    "\n",
    "        # Use explicit index positions instead of boolean indexing across dataframes\n",
    "        idx_to_imp = df_imp.index[to_imp]\n",
    "\n",
    "        neigh_vals = df_neigh.reindex(idx_to_imp)[var].values.reshape(-1, 1)\n",
    "        preds = m.predict(neigh_vals)\n",
    "\n",
    "        df_imp.loc[idx_to_imp, var] = preds\n",
    "\n",
    "        logs.append({\"var\": var,\n",
    "                     \"total_missing\": n_total,\n",
    "                     \"imputed_spatial\": n_imp,\n",
    "                     \"still_missing\": n_total - n_imp})\n",
    "\n",
    "    return df_imp, pd.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd025cd5-88a3-4959-8ca8-6be6f2188359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WVHT ===\n",
      "  total_missing: 6652, imputable with neighbour: 1332\n",
      "\n",
      "=== APD ===\n",
      "  total_missing: 6652, imputable with neighbour: 1332\n",
      "\n",
      "=== DPD ===\n",
      "  total_missing: 6652, imputable with neighbour: 1332\n",
      "\n",
      "=== WSPD ===\n",
      "  total_missing: 1028, imputable with neighbour: 1008\n",
      "\n",
      "=== WVHT ===\n",
      "  total_missing: 41526, imputable with neighbour: 763\n",
      "\n",
      "=== APD ===\n",
      "  total_missing: 41526, imputable with neighbour: 763\n",
      "\n",
      "=== DPD ===\n",
      "  total_missing: 41526, imputable with neighbour: 763\n",
      "\n",
      "=== WSPD ===\n",
      "  total_missing: 24, imputable with neighbour: 14\n",
      "\n",
      "=== WVHT ===\n",
      "  total_missing: 25426, imputable with neighbour: 8\n",
      "\n",
      "=== APD ===\n",
      "  total_missing: 25426, imputable with neighbour: 8\n",
      "\n",
      "=== DPD ===\n",
      "  total_missing: 25426, imputable with neighbour: 8\n",
      "\n",
      "=== WSPD ===\n",
      "  total_missing: 34, imputable with neighbour: 14\n"
     ]
    }
   ],
   "source": [
    "vars_spatial = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "df25_train_sp, log_train = apply_spatial(df25_train, df02_train, spatial_models, vars_spatial)\n",
    "df25_val_sp,   log_val   = apply_spatial(df25_val,   df02_val,   spatial_models, vars_spatial)\n",
    "df25_test_sp,  log_test  = apply_spatial(df25_test,  df02_test,  spatial_models, vars_spatial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e449e35-9417-4634-bcd7-7ba39669fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing % in train after spatial:\n",
      "WVHT    14.150442\n",
      "APD     14.150442\n",
      "DPD     14.150442\n",
      "WSPD     0.053197\n",
      "dtype: float64\n",
      "\n",
      "Missing % in val after spatial:\n",
      "WVHT    81.85177\n",
      "APD     81.85177\n",
      "DPD     81.85177\n",
      "WSPD     0.02008\n",
      "dtype: float64\n",
      "\n",
      "Missing % in test after spatial:\n",
      "WVHT    83.392388\n",
      "APD     83.392388\n",
      "DPD     83.392388\n",
      "WSPD     0.065617\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "core_vars = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "def missing_pct(df, name):\n",
    "    print(f\"\\nMissing % in {name}:\")\n",
    "    print(df[core_vars].isna().mean().mul(100))\n",
    "\n",
    "missing_pct(df25_train_sp, \"train after spatial\")\n",
    "missing_pct(df25_val_sp,   \"val after spatial\")\n",
    "missing_pct(df25_test_sp,  \"test after spatial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f702b3a-ad03-4c66-ad5d-6bc5bf5c8f23",
   "metadata": {},
   "source": [
    "### Final Missing-Data Strategy (Leakage-Free, Spatial + Temporal)\n",
    "\n",
    "Our goal is to use spatial information from Station 41002 to improve the 41025 record while avoiding data leakage and keeping the forecasting setup publication‑grade.\n",
    "\n",
    "We adopt the following plan:\n",
    "\n",
    "1. **Time-based splitting before imputation**\n",
    "\n",
    "   - We split the 41025 record into:\n",
    "     - Train: 2014‑01‑01 to 2017‑12‑31  \n",
    "     - Validation: 2018‑01‑01 to 2018‑12‑31  \n",
    "     - Test: 2019‑01‑01 to 2019‑12‑31  \n",
    "   - All imputation models are trained **only** on the train period and then applied to val and test, so no future information from 2018–2019 leaks into training.\n",
    "\n",
    "2. **Spatial imputation using neighbour 41002**\n",
    "\n",
    "   - On the train split, we build simple regression models of the form  \n",
    "     \\( \\text{var}_{41025}(t) \\approx f(\\text{var}_{41002}(t)) \\)  \n",
    "     for WVHT, APD, DPD, and WSPD using only overlapping train data.\n",
    "   - We then apply these frozen models to train, val, and test to fill values at 41025 **only when**:\n",
    "     - 41025 is missing, and  \n",
    "     - 41002 has an observation at the same timestamp.\n",
    "   - This step reduces missingness, especially for WSPD, and uses spatial information in a way that is realistic for real‑time deployment.\n",
    "\n",
    "3. **Short-gap temporal imputation in train only**\n",
    "\n",
    "   - After spatial imputation, about 14% of WVHT/APD/DPD in the train set are still missing.\n",
    "   - For the **training split only**, we apply short-gap temporal interpolation (e.g., up to 6 hours) to fill these remaining small gaps.\n",
    "   - Long gaps that cannot be reliably interpolated are left missing and later dropped.\n",
    "   - This makes the training data denser without affecting the integrity of validation and test evaluation.\n",
    "\n",
    "4. **Conservative handling of validation and test**\n",
    "\n",
    "   - For validation and test, we **do not** apply any additional temporal interpolation to the target beyond spatial imputation.\n",
    "   - We identify the longest continuous segments in 2018 and 2019 where WVHT is present (original or spatially imputed) and use those segments as our evaluation windows.\n",
    "   - Within these windows, we drop any timestamps where WVHT or key predictors (e.g., WSPD, PRES, ATMP, WTMP) are still missing.\n",
    "   - As a result, model performance is evaluated only on times where the target is genuinely observable (or spatially reconstructed), with no use of future values for imputation.\n",
    "\n",
    "5. **Summary**\n",
    "\n",
    "   - Train: spatial imputation (41002 → 41025) + short-gap temporal interpolation, then drop any remaining missing WVHT.\n",
    "   - Validation/Test: spatial imputation only; select continuous segments with available WVHT and drop remaining missing rows.\n",
    "   - The neighbour station is used **only** for imputation, never as a direct predictor in forecasting models.\n",
    "\n",
    "This strategy balances the use of spatial information, strict control of data leakage, and practical handling of high missingness, aligning with recent best practices in time-series imputation and forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb0f2d09-de8a-4586-bf31-73be9b604b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after cleaning: (37596, 13)\n",
      "Missing % in train after spatial + temporal:\n",
      "WVHT    0.0\n",
      "APD     0.0\n",
      "DPD     0.0\n",
      "WSPD    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "core_vars = [\"WVHT\", \"APD\", \"DPD\", \"WSPD\"]\n",
    "\n",
    "def temporal_impute_short_gaps(df, vars_list, max_gap=6):\n",
    "    df2 = df.copy()\n",
    "    for var in vars_list:\n",
    "        if var not in df2.columns:\n",
    "            continue\n",
    "        # Interpolate only short gaps up to max_gap points\n",
    "        df2[var] = df2[var].interpolate(limit=max_gap, limit_direction=\"both\")\n",
    "    return df2\n",
    "\n",
    "# 1.1 Apply short-gap interpolation on train after spatial\n",
    "df25_train_clean = temporal_impute_short_gaps(df25_train_sp, core_vars, max_gap=6)\n",
    "\n",
    "# 1.2 Drop any rows still missing WVHT in train\n",
    "df25_train_clean = df25_train_clean[df25_train_clean[\"WVHT\"].notna()].copy()\n",
    "\n",
    "print(\"Train shape after cleaning:\", df25_train_clean.shape)\n",
    "print(\"Missing % in train after spatial + temporal:\")\n",
    "print(df25_train_clean[core_vars].isna().mean().mul(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5b37717-2b4c-472a-9fab-c7c3cb3daa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:50:00</th>\n",
       "      <td>301.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1026.6</td>\n",
       "      <td>11.8</td>\n",
       "      <td>23.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:50:00</th>\n",
       "      <td>305.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1027.6</td>\n",
       "      <td>11.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:50:00</th>\n",
       "      <td>308.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>23.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 03:50:00</th>\n",
       "      <td>322.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 04:50:00</th>\n",
       "      <td>322.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1027.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:20:00</th>\n",
       "      <td>356.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.688670</td>\n",
       "      <td>6.736889</td>\n",
       "      <td>5.093255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:30:00</th>\n",
       "      <td>358.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.769335</td>\n",
       "      <td>6.148445</td>\n",
       "      <td>4.986627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:40:00</th>\n",
       "      <td>359.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1022.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:50:00</th>\n",
       "      <td>357.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.485490</td>\n",
       "      <td>7.017826</td>\n",
       "      <td>5.370779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 23:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.485490</td>\n",
       "      <td>7.017826</td>\n",
       "      <td>5.370779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1022.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37596 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      WDIR  WSPD   GST      WVHT       DPD       APD    MWD  \\\n",
       "datetime                                                                      \n",
       "2014-01-01 00:50:00  301.0   6.8   9.5  0.960000  6.250000  4.400000  360.0   \n",
       "2014-01-01 01:50:00  305.0   7.5   9.5  0.890000  5.880000  4.230000    2.0   \n",
       "2014-01-01 02:50:00  308.0   6.6   9.4  0.890000  5.880000  4.150000    3.0   \n",
       "2014-01-01 03:50:00  322.0   7.0  10.1  0.920000  6.250000  4.380000   26.0   \n",
       "2014-01-01 04:50:00  322.0   5.2   7.2  0.940000  5.880000  4.210000   14.0   \n",
       "...                    ...   ...   ...       ...       ...       ...    ...   \n",
       "2017-12-31 22:20:00  356.0  13.1  16.3  1.688670  6.736889  5.093255    NaN   \n",
       "2017-12-31 22:30:00  358.0  13.2  17.3  1.769335  6.148445  4.986627    NaN   \n",
       "2017-12-31 22:40:00  359.0  13.1  16.0  1.850000  5.560000  4.880000   24.0   \n",
       "2017-12-31 22:50:00  357.0  12.9  16.0  1.485490  7.017826  5.370779    NaN   \n",
       "2017-12-31 23:00:00    4.0  13.5  17.5  1.485490  7.017826  5.370779    NaN   \n",
       "\n",
       "                       PRES  ATMP  WTMP  DEWP  VIS  TIDE  \n",
       "datetime                                                  \n",
       "2014-01-01 00:50:00  1026.6  11.8  23.3   1.4  NaN   NaN  \n",
       "2014-01-01 01:50:00  1027.6  11.9  23.3   0.5  NaN   NaN  \n",
       "2014-01-01 02:50:00  1027.9  11.7  23.3  -0.1  NaN   NaN  \n",
       "2014-01-01 03:50:00  1027.9  12.0  23.2  -0.3  NaN   NaN  \n",
       "2014-01-01 04:50:00  1027.7  11.7  23.2  -2.1  NaN   NaN  \n",
       "...                     ...   ...   ...   ...  ...   ...  \n",
       "2017-12-31 22:20:00  1023.0   1.9  13.4  -2.4  NaN   NaN  \n",
       "2017-12-31 22:30:00  1023.0   1.8  13.4  -2.6  NaN   NaN  \n",
       "2017-12-31 22:40:00  1022.9   1.8  13.4  -2.7  NaN   NaN  \n",
       "2017-12-31 22:50:00  1023.0   1.9  13.5  -2.6  NaN   NaN  \n",
       "2017-12-31 23:00:00  1022.2   1.8  13.5  -2.4  NaN   NaN  \n",
       "\n",
       "[37596 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df25_train_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9ae19e1-b1d7-414c-a5a0-6d4a670897b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"VIS\", \"TIDE\"] \n",
    "cols_to_drop = [c for c in cols_to_drop if c in df25_train_clean.columns]\n",
    "\n",
    "df25_train_clean = df25_train_clean.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "804df85f-99f1-4ea7-b7a8-91a5d04e56c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:50:00</th>\n",
       "      <td>301.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1026.6</td>\n",
       "      <td>11.8</td>\n",
       "      <td>23.3</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:50:00</th>\n",
       "      <td>305.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1027.6</td>\n",
       "      <td>11.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:50:00</th>\n",
       "      <td>308.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>23.3</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 03:50:00</th>\n",
       "      <td>322.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1027.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 04:50:00</th>\n",
       "      <td>322.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1027.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:20:00</th>\n",
       "      <td>356.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.688670</td>\n",
       "      <td>6.736889</td>\n",
       "      <td>5.093255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:30:00</th>\n",
       "      <td>358.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.769335</td>\n",
       "      <td>6.148445</td>\n",
       "      <td>4.986627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:40:00</th>\n",
       "      <td>359.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1022.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:50:00</th>\n",
       "      <td>357.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.485490</td>\n",
       "      <td>7.017826</td>\n",
       "      <td>5.370779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 23:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.485490</td>\n",
       "      <td>7.017826</td>\n",
       "      <td>5.370779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1022.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37596 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      WDIR  WSPD   GST      WVHT       DPD       APD    MWD  \\\n",
       "datetime                                                                      \n",
       "2014-01-01 00:50:00  301.0   6.8   9.5  0.960000  6.250000  4.400000  360.0   \n",
       "2014-01-01 01:50:00  305.0   7.5   9.5  0.890000  5.880000  4.230000    2.0   \n",
       "2014-01-01 02:50:00  308.0   6.6   9.4  0.890000  5.880000  4.150000    3.0   \n",
       "2014-01-01 03:50:00  322.0   7.0  10.1  0.920000  6.250000  4.380000   26.0   \n",
       "2014-01-01 04:50:00  322.0   5.2   7.2  0.940000  5.880000  4.210000   14.0   \n",
       "...                    ...   ...   ...       ...       ...       ...    ...   \n",
       "2017-12-31 22:20:00  356.0  13.1  16.3  1.688670  6.736889  5.093255    NaN   \n",
       "2017-12-31 22:30:00  358.0  13.2  17.3  1.769335  6.148445  4.986627    NaN   \n",
       "2017-12-31 22:40:00  359.0  13.1  16.0  1.850000  5.560000  4.880000   24.0   \n",
       "2017-12-31 22:50:00  357.0  12.9  16.0  1.485490  7.017826  5.370779    NaN   \n",
       "2017-12-31 23:00:00    4.0  13.5  17.5  1.485490  7.017826  5.370779    NaN   \n",
       "\n",
       "                       PRES  ATMP  WTMP  DEWP  \n",
       "datetime                                       \n",
       "2014-01-01 00:50:00  1026.6  11.8  23.3   1.4  \n",
       "2014-01-01 01:50:00  1027.6  11.9  23.3   0.5  \n",
       "2014-01-01 02:50:00  1027.9  11.7  23.3  -0.1  \n",
       "2014-01-01 03:50:00  1027.9  12.0  23.2  -0.3  \n",
       "2014-01-01 04:50:00  1027.7  11.7  23.2  -2.1  \n",
       "...                     ...   ...   ...   ...  \n",
       "2017-12-31 22:20:00  1023.0   1.9  13.4  -2.4  \n",
       "2017-12-31 22:30:00  1023.0   1.8  13.4  -2.6  \n",
       "2017-12-31 22:40:00  1022.9   1.8  13.4  -2.7  \n",
       "2017-12-31 22:50:00  1023.0   1.9  13.5  -2.6  \n",
       "2017-12-31 23:00:00  1022.2   1.8  13.5  -2.4  \n",
       "\n",
       "[37596 rows x 11 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df25_train_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c47114e-6009-4afe-975c-20d6c19717c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top val segments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WVHT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-31 23:40:00</td>\n",
       "      <td>2017-12-31 23:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2018-01-21 19:40:00</td>\n",
       "      <td>2018-01-21 19:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>2018-01-21 10:40:00</td>\n",
       "      <td>2018-01-21 10:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2018-01-21 11:40:00</td>\n",
       "      <td>2018-01-21 11:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2018-01-21 12:40:00</td>\n",
       "      <td>2018-01-21 12:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   start                 end  length\n",
       "WVHT                                                \n",
       "2    2017-12-31 23:40:00 2017-12-31 23:50:00       2\n",
       "1002 2018-01-21 19:40:00 2018-01-21 19:50:00       2\n",
       "984  2018-01-21 10:40:00 2018-01-21 10:50:00       2\n",
       "986  2018-01-21 11:40:00 2018-01-21 11:50:00       2\n",
       "988  2018-01-21 12:40:00 2018-01-21 12:50:00       2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top test segments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WVHT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2019-06-04 10:40:00</td>\n",
       "      <td>2019-06-04 10:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>2019-10-22 09:40:00</td>\n",
       "      <td>2019-10-22 09:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6760</th>\n",
       "      <td>2019-10-22 16:40:00</td>\n",
       "      <td>2019-10-22 16:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>2019-10-22 15:40:00</td>\n",
       "      <td>2019-10-22 15:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>2019-10-22 14:40:00</td>\n",
       "      <td>2019-10-22 14:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   start                 end  length\n",
       "WVHT                                                \n",
       "160  2019-06-04 10:40:00 2019-06-04 10:50:00       2\n",
       "6746 2019-10-22 09:40:00 2019-10-22 09:40:00       1\n",
       "6760 2019-10-22 16:40:00 2019-10-22 16:40:00       1\n",
       "6758 2019-10-22 15:40:00 2019-10-22 15:40:00       1\n",
       "6756 2019-10-22 14:40:00 2019-10-22 14:40:00       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def longest_segment(df, col):\n",
    "    mask = df[col].notna()\n",
    "    seg_id = (mask != mask.shift()).cumsum()\n",
    "    segments = (\n",
    "        df[mask]\n",
    "        .groupby(seg_id[mask])\n",
    "        .agg(start=(col, lambda s: s.index.min()),\n",
    "             end=(col,   lambda s: s.index.max()),\n",
    "             length=(col, \"size\"))\n",
    "        .sort_values(\"length\", ascending=False)\n",
    "    )\n",
    "    return segments\n",
    "\n",
    "seg_val  = longest_segment(df25_val_sp,  \"WVHT\")\n",
    "seg_test = longest_segment(df25_test_sp, \"WVHT\")\n",
    "\n",
    "print(\"Top val segments:\")\n",
    "display(seg_val.head())\n",
    "print(\"Top test segments:\")\n",
    "display(seg_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f503a89-3e3e-4928-b991-5b44cbc5780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val window: 2017-12-31 23:40:00 → 2017-12-31 23:50:00 rows: 49797\n",
      "Test window: 2019-06-04 10:40:00 → 2019-06-04 10:50:00 rows: 2\n"
     ]
    }
   ],
   "source": [
    "val_start  = seg_val.iloc[0][\"start\"]\n",
    "val_end    = seg_val.iloc[0][\"end\"]\n",
    "test_start = seg_test.iloc[0][\"start\"]\n",
    "test_end   = seg_test.iloc[0][\"end\"]\n",
    "\n",
    "df25_val_clean  = df25_val_sp.loc[val_start:test_end].copy()   # or val_start:val_end\n",
    "df25_test_clean = df25_test_sp.loc[test_start:test_end].copy()\n",
    "\n",
    "print(\"Val window:\", val_start, \"→\", val_end,  \"rows:\", len(df25_val_clean))\n",
    "print(\"Test window:\", test_start, \"→\", test_end, \"rows:\", len(df25_test_clean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510057b-1994-4804-a01d-67b9a2a2ec90",
   "metadata": {},
   "source": [
    "### From Spatial Imputation to Final Modeling Strategy\n",
    "\n",
    "At this stage, we have:\n",
    "\n",
    "- Split the 41025 time series into three calendar periods:\n",
    "  - 2014–2017 (initially planned as train),\n",
    "  - 2018 (initially planned as validation),\n",
    "  - 2019 (initially planned as test).\n",
    "- Built **spatial regression models** using Station 41002 to predict missing values at Station 41025.\n",
    "- Applied these models to each split, filling any timestamp where 41025 was missing but 41002 was observed.\n",
    "- Measured the remaining missingness in the key variables (WVHT, APD, DPD, WSPD) after spatial imputation:\n",
    "  - Train (2014–2017): about 14% missing in WVHT/APD/DPD, almost 0% in WSPD.\n",
    "  - Val (2018): about 82% missing in WVHT/APD/DPD.\n",
    "  - Test (2019): about 83% missing in WVHT/APD/DPD.\n",
    "\n",
    "We then searched for **continuous segments of non-missing WVHT** in 2018 and 2019 and found that:\n",
    "\n",
    "- The longest WVHT segments in 2018 and 2019 are only **2 hours long**.\n",
    "- This means there is essentially **no substantial continuous WVHT record** available for quantitative validation or testing at Station 41025 during 2018–2019, even after spatial imputation.\n",
    "\n",
    "Because of this, using 2018–2019 as formal validation and test sets would force us to evaluate forecasting models on mostly imputed targets, which is statistically weak and not suitable.\n",
    "\n",
    "### Final Methods We Adopt Going Forward\n",
    "\n",
    "To ensure a rigorous and defensible forecasting study, we adopt the following strategy:\n",
    "\n",
    "1. **Core modeling dataset (2014–2017)**  \n",
    "   - We focus on the 2014–2017 period at Station 41025, where spatial imputation plus short-gap temporal interpolation produce a dense and reliable record.\n",
    "   - After:\n",
    "     - applying neighbour-based spatial imputation from 41002, and\n",
    "     - applying short-gap temporal interpolation (up to a small number of hours) in 2014–2017,\n",
    "     - we removed any remaining rows with missing WVHT.\n",
    "   - The resulting dataset `df25_train_clean` has:\n",
    "     - 0% missing WVHT,\n",
    "     - 0% missing APD, DPD, and WSPD,\n",
    "     - and a complete set of meteorological predictors (PRES, ATMP, WTMP, MWD, WDIR, etc.).\n",
    "   - This clean dataset forms the **basis for all model training and quantitative evaluation**.\n",
    "\n",
    "2. **Rolling time-series cross-validation within 2014–2017**  \n",
    "   - Instead of using 2018 and 2019 as standard validation/test years, we evaluate models using **rolling cross-validation** on the 2014–2017 period.  \n",
    "   - Example folds:\n",
    "     - Fold 1: train on 2014–2015, validate on 2016.\n",
    "     - Fold 2: train on 2014–2016, validate on 2017.\n",
    "   - This approach:\n",
    "     - preserves temporal order,\n",
    "     - uses only real (or spatially reconstructed) WVHT at 41025,\n",
    "     - and provides robust performance estimates without relying on sparsely observed future years.\n",
    "\n",
    "3. **Role of Station 41002 (spatial imputation only)**  \n",
    "   - Station 41002 is used **only** for spatial imputation, not as a predictor in forecasting models.\n",
    "   - Spatial regression models 41002 → 41025 are trained solely on 2014–2017 (train years) and then applied within that period to fill gaps.\n",
    "   - We quantify the effect of this step by reporting:\n",
    "     - the reduction in missingness before and after spatial imputation,\n",
    "     - and the imputation accuracy metrics (MAE, R²) on the overlapping train data.\n",
    "   - This spatial imputation step is a key methodological contribution of the study.\n",
    "\n",
    "4. **Use of 2018–2019 data**  \n",
    "   - Because 2018 and 2019 at 41025 contain almost no continuous WVHT segments, they are **not used as formal validation or test sets**.\n",
    "   - Instead, we may use any short periods where WVHT is available as:\n",
    "     - small illustrative case studies or plots,\n",
    "     - clearly labeled as qualitative checks rather than the main performance benchmarks.\n",
    "   - This keeps the evaluation honest and avoids drawing strong conclusions from heavily imputed or extremely sparse data.\n",
    "\n",
    "In summary, the team will move forward by:\n",
    "\n",
    "- Training and evaluating all forecasting models on the **clean 2014–2017 dataset at Station 41025** using rolling time-series cross-validation, and\n",
    "- Presenting **spatial imputation from Station 41002** as a central methodological component that improves data quality without introducing data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f3bdea4-e461-400c-8936-f170aea9a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df25_train_clean summary\n",
      "----------------------------------------\n",
      "Index range: 2014-01-01 00:50:00 → 2017-12-31 23:00:00\n",
      "Rows       : 37596\n",
      "Columns    : ['WDIR', 'WSPD', 'GST', 'WVHT', 'DPD', 'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP']\n"
     ]
    }
   ],
   "source": [
    "# Confirm and freeze the core modeling dataset (2014–2017)\n",
    "\n",
    "print(\"df25_train_clean summary\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Index range:\", df25_train_clean.index.min(), \"→\", df25_train_clean.index.max())\n",
    "print(\"Rows       :\", len(df25_train_clean))\n",
    "print(\"Columns    :\", df25_train_clean.columns.tolist())\n",
    "\n",
    "# Optional: save a frozen copy so everyone uses the same file\n",
    "df25_train_clean.to_parquet(\"df25_train_clean_2014_2017.parquet\")\n",
    "# df25_train_clean.to_csv(\"df25_train_clean_2014_2017.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd337ca1-5ebd-4a7e-9a27-87dbac7bd290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base feature columns (41025 only):\n",
      "['WVHT', 'APD', 'DPD', 'WSPD', 'GST', 'MWD', 'WDIR', 'PRES', 'ATMP', 'WTMP']\n"
     ]
    }
   ],
   "source": [
    "# Define target and predictor variables (NO neighbour station columns)\n",
    "\n",
    "# Primary forecasting target\n",
    "TARGET_COL = \"WVHT\"  # significant wave height at station 41025\n",
    "\n",
    "# Core dynamic predictors (all at 41025, already cleaned)\n",
    "base_feature_cols = [\n",
    "    \"WVHT\", \"APD\", \"DPD\",      # wave characteristics\n",
    "    \"WSPD\", \"GST\",             # wind\n",
    "    \"MWD\", \"WDIR\",             # directions\n",
    "    \"PRES\", \"ATMP\", \"WTMP\",    # met variables\n",
    "]\n",
    "\n",
    "# Keep only columns we actually want to use \n",
    "available_features = [c for c in base_feature_cols if c in df25_train_clean.columns]\n",
    "print(\"Base feature columns (41025 only):\")\n",
    "print(available_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13cfc9f4-b678-495b-94dd-341b1155a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagged dataset shape: (18862, 241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_56256\\3524349553.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WVHT_t_plus_1</th>\n",
       "      <th>WVHT_lag_1</th>\n",
       "      <th>WVHT_lag_2</th>\n",
       "      <th>WVHT_lag_3</th>\n",
       "      <th>WVHT_lag_4</th>\n",
       "      <th>WVHT_lag_5</th>\n",
       "      <th>WVHT_lag_6</th>\n",
       "      <th>WVHT_lag_7</th>\n",
       "      <th>WVHT_lag_8</th>\n",
       "      <th>WVHT_lag_9</th>\n",
       "      <th>...</th>\n",
       "      <th>WTMP_lag_15</th>\n",
       "      <th>WTMP_lag_16</th>\n",
       "      <th>WTMP_lag_17</th>\n",
       "      <th>WTMP_lag_18</th>\n",
       "      <th>WTMP_lag_19</th>\n",
       "      <th>WTMP_lag_20</th>\n",
       "      <th>WTMP_lag_21</th>\n",
       "      <th>WTMP_lag_22</th>\n",
       "      <th>WTMP_lag_23</th>\n",
       "      <th>WTMP_lag_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02 00:50:00</th>\n",
       "      <td>1.141698</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 01:50:00</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-04 09:50:00</th>\n",
       "      <td>2.210000</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.42</td>\n",
       "      <td>...</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-04 10:50:00</th>\n",
       "      <td>2.230000</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-04 11:50:00</th>\n",
       "      <td>2.380000</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.28</td>\n",
       "      <td>...</td>\n",
       "      <td>22.6</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     WVHT_t_plus_1  WVHT_lag_1  WVHT_lag_2  WVHT_lag_3  \\\n",
       "datetime                                                                 \n",
       "2014-01-02 00:50:00       1.141698        0.66        0.62        0.68   \n",
       "2014-01-02 01:50:00       0.850000        0.62        0.66        0.62   \n",
       "2014-01-04 09:50:00       2.210000        2.15        1.94        2.06   \n",
       "2014-01-04 10:50:00       2.230000        2.13        2.15        1.94   \n",
       "2014-01-04 11:50:00       2.380000        2.21        2.13        2.15   \n",
       "\n",
       "                     WVHT_lag_4  WVHT_lag_5  WVHT_lag_6  WVHT_lag_7  \\\n",
       "datetime                                                              \n",
       "2014-01-02 00:50:00        0.65        0.64        0.67        0.69   \n",
       "2014-01-02 01:50:00        0.68        0.65        0.64        0.67   \n",
       "2014-01-04 09:50:00        2.18        2.40        2.27        2.28   \n",
       "2014-01-04 10:50:00        2.06        2.18        2.40        2.27   \n",
       "2014-01-04 11:50:00        1.94        2.06        2.18        2.40   \n",
       "\n",
       "                     WVHT_lag_8  WVHT_lag_9  ...  WTMP_lag_15  WTMP_lag_16  \\\n",
       "datetime                                     ...                             \n",
       "2014-01-02 00:50:00        0.74        0.69  ...         23.1         23.2   \n",
       "2014-01-02 01:50:00        0.69        0.74  ...         23.1         23.1   \n",
       "2014-01-04 09:50:00        2.03        2.42  ...         22.7         22.7   \n",
       "2014-01-04 10:50:00        2.28        2.03  ...         22.5         22.7   \n",
       "2014-01-04 11:50:00        2.27        2.28  ...         22.6         22.5   \n",
       "\n",
       "                     WTMP_lag_17  WTMP_lag_18  WTMP_lag_19  WTMP_lag_20  \\\n",
       "datetime                                                                  \n",
       "2014-01-02 00:50:00         23.2         23.2         23.2         23.2   \n",
       "2014-01-02 01:50:00         23.2         23.2         23.2         23.2   \n",
       "2014-01-04 09:50:00         22.7         22.9         23.0         22.9   \n",
       "2014-01-04 10:50:00         22.7         22.7         22.9         23.0   \n",
       "2014-01-04 11:50:00         22.7         22.7         22.7         22.9   \n",
       "\n",
       "                     WTMP_lag_21  WTMP_lag_22  WTMP_lag_23  WTMP_lag_24  \n",
       "datetime                                                                 \n",
       "2014-01-02 00:50:00         23.2         23.3         23.3         23.3  \n",
       "2014-01-02 01:50:00         23.2         23.2         23.3         23.3  \n",
       "2014-01-04 09:50:00         22.8         22.5         22.5         22.6  \n",
       "2014-01-04 10:50:00         22.9         22.8         22.5         22.5  \n",
       "2014-01-04 11:50:00         23.0         22.9         22.8         22.5  \n",
       "\n",
       "[5 rows x 241 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create lagged features (autoregressive + exogenous lags)\n",
    "\n",
    "def make_lagged_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_cols: list,\n",
    "    max_lag: int = 24,\n",
    "    forecast_horizon: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Turn a time-indexed DataFrame into a supervised-learning table.\n",
    "\n",
    "    - target_col: column to forecast (e.g., WVHT)\n",
    "    - feature_cols: columns from which to create lags (all at station 41025)\n",
    "    - max_lag: largest lag in hours (e.g., 24 means 1–24h lags)\n",
    "    - forecast_horizon: how many hours ahead to predict (1 = t+1)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Target shifted into the future (what we want to predict)\n",
    "    y = df[target_col].shift(-forecast_horizon).rename(f\"{target_col}_t_plus_{forecast_horizon}\")\n",
    "\n",
    "    # Build lag features\n",
    "    X_lagged = pd.DataFrame(index=df.index)\n",
    "    for col in feature_cols:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            X_lagged[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "    # Combine and drop rows with any NaNs created by shifting\n",
    "    data = pd.concat([y, X_lagged], axis=1)\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "# 1-hour-ahead forecasting, lags up to 24 hours\n",
    "MAX_LAG = 24\n",
    "HORIZON = 1\n",
    "\n",
    "lagged_data = make_lagged_dataset(\n",
    "    df=df25_train_clean,\n",
    "    target_col=TARGET_COL,\n",
    "    feature_cols=available_features,\n",
    "    max_lag=MAX_LAG,\n",
    "    forecast_horizon=HORIZON,\n",
    ")\n",
    "\n",
    "print(\"Lagged dataset shape:\", lagged_data.shape)\n",
    "lagged_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec0112-2614-47aa-b12d-dd178b5f6623",
   "metadata": {},
   "source": [
    "## Rolling time‑series cross‑validation (2014–2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e2052e2-c592-43d2-84f9-3d13a6e321da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Rolling forecasting origin CV on the lagged dataset\n",
    "\n",
    "def rolling_year_splits(index: pd.DatetimeIndex):\n",
    "    \"\"\"\n",
    "    Generator of (train_mask, val_mask) for yearly expanding-window CV.\n",
    "\n",
    "    - Fold 1: train ≤ 2015-12-31, validate 2016\n",
    "    - Fold 2: train ≤ 2016-12-31, validate 2017\n",
    "    (all within 2014–2017)\n",
    "    \"\"\"\n",
    "    for val_year in [2016, 2017]:\n",
    "        train_end_year = val_year - 1\n",
    "        train_mask = index.year <= train_end_year\n",
    "        val_mask = index.year == val_year\n",
    "        yield f\"train_≤{train_end_year}_val_{val_year}\", train_mask, val_mask\n",
    "\n",
    "\n",
    "def evaluate_model_rolling_cv(model, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Apply rolling-year CV to a given sklearn-like model.\n",
    "\n",
    "    Returns a DataFrame with fold metrics and prints overall mean.\n",
    "    \"\"\"\n",
    "    idx = data.index\n",
    "\n",
    "    # identify target column name programmatically\n",
    "    target_cols = [c for c in data.columns if c.startswith(TARGET_COL + \"_t_plus_\")]\n",
    "    assert len(target_cols) == 1\n",
    "    target_col = target_cols[0]\n",
    "\n",
    "    # predictors = all other columns (all lag features)\n",
    "    X_all = data.drop(columns=[target_col])\n",
    "    y_all = data[target_col]\n",
    "\n",
    "    results = []\n",
    "    for fold_name, train_mask, val_mask in rolling_year_splits(idx):\n",
    "        X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "        X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
    "\n",
    "        print(f\"\\n=== {fold_name} ===\")\n",
    "        print(\"Train rows:\", len(X_train), \" Val rows:\", len(X_val))\n",
    "\n",
    "        mdl = model.__class__(**model.get_params())  # fresh clone\n",
    "        mdl.fit(X_train, y_train)\n",
    "        y_pred = mdl.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "        print(f\"MAE : {mae:.3f}\")\n",
    "        print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "        results.append({\"fold\": fold_name, \"MAE\": mae, \"RMSE\": rmse})\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    print(\"\\nOverall CV performance:\")\n",
    "    print(res_df)\n",
    "    print(\"\\nMean MAE :\", res_df[\"MAE\"].mean())\n",
    "    print(\"Mean RMSE:\", res_df[\"RMSE\"].mean())\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acc288-2c2f-43f2-a6b5-d85ff12638ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
